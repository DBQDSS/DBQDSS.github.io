

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.jpg">
  <link rel="icon" href="/img/logo.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="失去理想的獾">
  <meta name="keywords" content="">
  
    <meta name="description" content="一篇 arxiv 上的综述">
<meta property="og:type" content="article">
<meta property="og:title" content="A Survey on DLMs">
<meta property="og:url" content="http://dbqdss.github.io/2025/10/18/%E8%AF%BB%E8%AE%BA%E6%96%87/A-Survey-on-DLMs/index.html">
<meta property="og:site_name" content="失去理想的獾 の Blog">
<meta property="og:description" content="一篇 arxiv 上的综述">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://dbqdss.github.io/images/index-stack/20251018A-Survey-on-DLMs.png">
<meta property="article:published_time" content="2025-10-18T06:16:39.000Z">
<meta property="article:modified_time" content="2025-10-22T11:22:58.715Z">
<meta property="article:author" content="失去理想的獾">
<meta property="article:tag" content="Diffusion Model">
<meta property="article:tag" content="DLMs">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://dbqdss.github.io/images/index-stack/20251018A-Survey-on-DLMs.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet">
  
  <title>A Survey on DLMs - 失去理想的獾 の Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/KaTeX/0.16.2/katex.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/mac.css">
<link rel="stylesheet" href="/css/custom.css">
<link rel="stylesheet" href="/css/mouse.css">
<link rel="stylesheet" href="/css/font.css">
<link rel="stylesheet" href="/css/equation.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"dbqdss.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1, h2, h3, h4, h5, h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="失去理想的獾 の Blog" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>失去理想的獾 の Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/about/" target="_self">
                    <i class="iconfont icon-user-fill"></i>
                    <span>关于我</span>
                  </a>
                
                  
                  
                  
                  <a class="dropdown-item" href="/cv/my-cv.pdf" target="_self">
                    <i class="iconfont icon-addrcard"></i>
                    <span>简历</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/atom.xml" target="_self">
                <i class="iconfont icon-rss-fill"></i>
                <span>RSS</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/banner.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="A Survey on Diffusion Language Models"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        失去理想的獾
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-10-18 14:16" pubdate>
          2025年10月18日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          28k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          233 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        

      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">A Survey on Diffusion Language Models</h1>
            
              <p id="updated-time" class="note note-info" style="">
                
                  
                    本文最后更新于 2025年10月22日 晚上
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.10875">https://arxiv.org/abs/2508.10875</a><br>
机翻 + 笔记，作图书馆用。</p>
</blockquote>
<h2 id="0-Abstract">0. Abstract</h2>
<p><span style="color:cyan">扩散语言模型</span> (Diffusion Language Models, DLMs) 正迅速崛起，成为当前主流 <span style="color:cyan">自回归</span> (autoregressive, AR) 范式极具实力且前景广阔的替代方案。通过迭代去噪过程并行生成令牌 (token)，扩散语言模型在降低推理延迟和捕捉双向上下文方面具有内在优势，进而能够对生成过程实现细粒度控制。尽管已实现数倍的推理速度提升，近年来的技术进展仍使扩散语言模型展现出与自回归模型相当的性能，这使其成为各类自然语言处理任务中极具吸引力的选择。</p>
<p>尽管扩散语言模型的应用日益广泛，但该领域仍存在诸多值得进一步探索的挑战与机遇，这需要研究者对其原理、技术和局限性形成详尽且系统的认知。本综述对当前扩散语言模型的研究现状进行了全面概述：追溯了扩散语言模型的发展历程及其与自回归模型、<span style="color:cyan">掩码语言模型</span> (masked language models, MLMs) 等其他范式的关联，同时涵盖了该领域的基础原理与最先进模型。</p>
<p>本研究的贡献包括：</p>
<ol>
<li>提出了一个最新的、全面的扩散语言模型分类体系，并对现有技术（从<span style="color:lightgreen">预训练</span> (pre-training) 策略到先进的 <span style="color:lightgreen">后训练</span> (post-training) 方法）进行了深入分析；</li>
<li>对扩散语言模型的 <span style="color:lightgreen">推理策略</span> (inference strategies) 与优化方法展开了详尽综述，包括 <span style="color:lightgreen">解码并行性</span> (decoding parallelism) 提升、<span style="color:lightgreen">缓存机制</span> (caching<br>
mechanisms) 优化及 <span style="color:lightgreen">生成质量</span> (generation quality) 改进等方向；</li>
<li>重点介绍了扩散语言模型向多模态领域扩展的最新方法，并阐述了其在各类实际场景 (pracial scenatios) 中的应用。</li>
<li>本综述还探讨了扩散语言模型面临的局限性与挑战（包括效率问题、<span style="color:lightgreen">长序列处理</span> (long-sequence handling) 能力及基础设施需求等），同时勾勒了未来的研究方向，以推动这一快速发展领域的持续进步。</li>
</ol>
<p>本研究相关项目的 GitHub 仓库地址为：<a target="_blank" rel="noopener" href="https://github.com/VILA-Lab/Awesome-DLMs">https://github.com/VILA-Lab/Awesome-DLMs</a>.</p>
<h2 id="1-Introduction">1. Introduction</h2>
<p><span style="color:cyan">通用人工智能</span> (AGI) 的近期发展在很大程度上得益于 <span style="color:cyan">自回归大语言模型</span> (LLMs)<sup>[1]-[7]</sup> 以及用于图像和视频生成的 <span style="color:cyan">扩散模型</span><sup>[8]-[12]</sup> 的兴起。这些模型在多种模态的理解与生成任务中均展现出卓越能力，实现了以往难以想象的性能水平。这些模型的规模前所未有——体现在海量的参数数量、庞大的数据集、大量的训练投入以及推理过程中极高的计算需求——这将人工智能推向了新的高度，使这些模型具备了广博的通用知识，并能深刻理解语言与现实世界。</p>
<p>GPT 系列模型<sup>[1],[13],[14]</sup> 的崛起，尤其是 ChatGPT<sup>[2]</sup> 的公开发布，推动自回归 (AR) 语言模型在 NLP 领域占据了主导地位。通过 <span style="color:lightgreen">因果注意力</span> (causal attention) 和 <span style="color:lightgreen">教师强制机制</span> (teacher forcing) 训练模型预测下一个 token，AR 模型<sup>[4],[15],[16]</sup> 能够有效扩展至大规模数据集与模型规模。</p>
<p>AR 模型 <span style="color:lightgreen">以逐 token 的顺序方式生成文本</span>，擅长支持各类任务，从简单的问答到复杂的推理及创意写作均有出色表现。然而，这种顺序生成的特性给推理速度带来了 <span style="color: red;">重大瓶颈</span>：自回归生成过程每次仅能生成一个 token，这一本质属性 <span style="color:lightgreen">限制了并行计算能力</span>，严重制约了计算效率与吞吐量。</p>
<p><span style="color:cyan">扩散模型</span> 是另一种极具前景的生成范式。这类模型通过训练，能够从逐渐加入噪声的数据中，通过去噪过程恢复原始数据，并通过逐步逆转这种随机损坏过程来生成新样本。<span style="color:cyan">扩散模型</span> 在建模复杂数据分布方面表现突出，已在图像和视频合成任务中取得了最先进 (state-of-the-art, sota) 的成果<sup>[17]</sup>。</p>
<p>扩散建模领域的学术突破<sup>[18]-[21]</sup> 为模型的训练与推理奠定了坚实的理论基础。与此同时，Stable Diffusion<sup>[8],[10],[11]</sup>、Imagen<sup>[9]</sup>、Sora<sup>[12]</sup> 等大规模实用模型，充分展现了扩散范式卓越的可扩展性与泛化能力——仅需简单的文本提示（往往只需几个词），就能生成高保真度、艺术级别的图像与视频。除了强大的复杂数据分布建模能力，<span style="color:cyan">扩散模型</span> 在 <span style="color:lightgreen">并行计算</span> 方面还具有内在优势：通过迭代去噪过程，它们能够同时生成多个 token 乃至完整序列，这有望实现更优的推理吞吐量，并能更好地利用现代并行计算硬件。尽管仍面临挑战（尤其是在离散数据建模与动态序列长度处理方面），但扩散语言模型 (DLMs) 已成为一种极具吸引力的替代方案，可有效平衡生成质量与速度之间的权衡关系。</p>
<p>为使 <span style="color:cyan">扩散模型</span> 适配离散语言数据，研究者提出了多种关键方法。早期，<span style="color:cyan">扩散模型</span> 在图像合成等 <span style="color:lightgreen">连续域</span> 的成功，是推动 DLMs 发展的主要动力。</p>
<p>连续空间 DLMs 会将 token 映射为 <span style="color:lightgreen">嵌入向量</span> (embeddings)，并在连续空间中执行去噪过程，这一思路在 Diffusion-LM<sup>[22]</sup>、SED<sup>[23]</sup> 等开创性研究中已有体现。另一方面，<span style="color:cyan">离散空间扩散语言模型</span> (Discrete DLMs) 直接在 token 空间中定义扩散过程。早期研究（如 D3PM [24]）引入了 <span style="color:lightgreen">含吸收态</span> (absorbing states) 的结构化转移矩阵，支持 token 级别的加噪与去噪。后续研究（如 DiffusionBERT [25]）则整合了 <span style="color:cyan">预训练掩码语言模型</span>（如 BERT）以提升去噪质量，并提出了 <span style="color:lightgreen">定制化噪声调度</span>（如 spindle schedule），使 <span style="color:lightgreen">token 污染过程</span> (token corruption) 与 <span style="color:lightgreen">token 出现频率</span> 更好地对齐。这些早期模型证明了将迭代去噪应用于非自回归文本生成的可行性，同时具备可控性与并行性优势，但性能仍落后于性能强劲的自回归基准模型。</p>
<p>随着扩散语言模型的核心挑战逐步得到解决、该范式日趋成熟，更大规模的扩散语言模型已被开发。通过从自回归模型初始化参数，Dream [26]、DiffuLLaMA [27] 等 7B 参数级模型表明：扩散语言模型可基于现有模型高效适配，同时实现具有竞争力的性能。LLaDA-8B [28] 模型进一步证明了从零训练扩散语言模型的潜力，其性能可与同规模的 LLaMA3-8B 模型相媲美。</p>
<p><span style="color:cyan">多模态扩散语言模型</span> (Multimodal DLMs, aka. diffusion multimodal large language models, dMLLMs) 在建模文本、图像等混合数据方面也展现出潜力。这类模型基于开源扩散语言模型构建，例如LLaDA-V [29]、Dimple [30]、MMaDA [31] 等，它们将跨模态推理与生成能力整合到扩散框架中。与此同时，工业界对扩散语言模型的关注度也在不断提升：Mercury系列 [32]、Gemini Diffusion [33] 等模型不仅表现出强劲性能，还实现了每秒数千 token 的推理速度。这些进展凸显了扩散语言模型日益增长的实用性与商业潜力。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510181502729.png" srcset="/img/loading.gif" lazyload alt="Fig.1" width="720" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Fig.1: 扩散语言模型发展 timeline。该图展示了扩散语言模型发展历程中的关键里程碑事件，主要分为三大类别：连续型扩散语言模型、离散型扩散语言模型以及近期兴起的多模态扩散语言模型。值得注意的是，虽然早期研究主要集中在连续型扩散语言模型上，但近年来离散型扩散语言模型正获得越来越多的关注。
    </span>
  </div>
</div>
<p>我们在 Fig.1 中提供了扩散语言模型的发展时间线（涵盖代表性模型及最新进展 [34]-[40]），随后在 Fig.2 中展示了扩散语言模型的发展趋势可视化结果。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510181538869.png" srcset="/img/loading.gif" lazyload alt="Fig.2" width="500" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Fig.2: 扩散语言模型论文趋势图。关于离散型 DLM，统计数据源自引用 D3PM 的文献，并进一步筛选标题或摘要含"language"关键词的论文。连续型DLM的统计则基于本文关联知识库记载的相关研究数量。结果显示该领域研究热度持续攀升。注：本统计数据仅供参考。
    </span>
  </div>
</div>
<p>扩散语言模型在训练与推理阶段还面临独特的挑战与机遇。在训练阶段：</p>
<ul>
<li><strong>预训练</strong> (Pretraining)：通常采用与自回归语言模型或图像扩散模型类似的策略 [26]、[30]、[31]。为加快训练速度并复用已有训练成果，许多扩散语言模型会从预训练自回归模型的权重初始化 [26]、[27]。</li>
<li><strong>监督微调</strong> (Supervised fine-tuning, SFT)：扩散语言模型的监督微调流程也与自回归模型类似——向模型提供干净的提示词（prompt）数据，模型通过学习生成目标补全内容。</li>
<li><strong>强化学习</strong> (Reinforcement learning, RL)：该技术也被应用于扩散语言模型的后训练阶段，以提升模型在复杂任务上的性能。研究者已提出 GRPO [41] 算法的多种变体（如 diffu-GRPO [42]、UniGRPO [31]），用于增强模型的推理能力，并在大规模场景下实现扩散语言模型 (DLMs) 的对齐。</li>
</ul>
<p>在推理阶段，研究者已开发出多种策略与优化方法，以充分发挥 DLMs 的性能：</p>
<ul>
<li>连续空间 DLMs 可利用 ODE/SDE 求解器或其他少步生成技术，加速迭代去噪过程 [43]；</li>
<li>由于离散空间 DLMs 在并行生成方面面临更多挑战，研究者提出了专门的并行解码策略 [30]、[44]、[45]，以实现单步接收多个 token 并克服“并行诅咒” (parallel curse)；</li>
<li>解掩码与重掩码策略[28]、[46] 通过选择性暴露低置信度 token，进一步提升生成质量；</li>
<li>缓存技术[47]、[48] 则可显著减少两种范式（连续/离散 DLMs）的计算量，同时提升推理速度。</li>
</ul>
<p>与自回归模型相比，<span style="color:cyan">扩散语言模型</span> 被广泛认为具有以下几方面 <span style="color:red">显著优势</span>：</p>
<ol>
<li><strong>并行生成能力</strong> (Parallel Generation)：通过迭代去噪过程，DLMs 可并行生成多个 token，推理速度与吞吐量较自回归模型有显著提升；</li>
<li><strong>双向上下文建模</strong> (Bidirectional Context)：DLMs 天然具备双向上下文整合能力，能实现更细腻的语言理解与生成，同时生成更丰富的上下文嵌入向量——这不仅有利于跨模态生成任务，还能对生成过程实现细粒度控制；</li>
<li><strong>迭代优化机制</strong> (Iterative Refinement)：迭代去噪过程允许 DLMs 在多步迭代中不断更新对序列的认知。对于掩码类 DLMs而言，它们可早期确定高置信度 token，并将低置信度区域保留为掩码状态，逐步优化不确定部分，最终生成更连贯、更高质量的文本；</li>
<li><strong>生成可控性</strong> (Controllability)：DLMs 可基于特定 token 位置或结构进行条件生成，因此非常适合文本填充、结构化生成等任务；此外，引导技术（如无分类器引导，classifier-free guidance）还能更好地控制生成文本的风格与语义相关性；</li>
<li><strong>跨模态统一建模</strong> (Unified Modeling Across Modalities)：借助共享的“基于去噪”建模框架，DLMs 天然支持文本与视觉的统一生成任务，这使其在“单模型需同时实现生成与理解”的多模态应用中极具潜力。</li>
</ol>
<p>尽管 DLMs 近年来关注度快速上升，但目前仍缺乏一份能系统涵盖整个 DLM 生态的综合性综述。为此，本综述的结构安排如下：</p>
<ul>
<li>第2章：全面概述现代语言建模范式，包括自回归、掩码及基于扩散的建模方法；</li>
<li>第3章：深入探讨 DLMs 的训练方法，涵盖 Pretraining、SFT、RL 对齐等后续微调技术；</li>
<li>第4章：详细阐述各类推理策略与优化方法，重点介绍针对连续空间与离散空间DLMs 的定制化技术；</li>
<li>第5章：探索扩散模型向多模态领域的扩展，综述 LLaDA-V[29]、MMaDA[31]、Dimple[30]等最先进模型与架构；</li>
<li>第6章：呈现并可视化 DLMs 的性能对比结果；</li>
<li>第7章：展示 DLMs 的多样化应用场景，涵盖文本生成、代码生成、计算生物学等任务；</li>
<li>第8章：重点分析 DLMs 面临的挑战与局限性（包括效率、推理能力、智能体能力、基础设施等问题），并勾勒未来研究的潜在方向。</li>
</ul>
<p>为提供整体概览，Fig.3 展示了 DLMs 的分类体系。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510181712589.png" srcset="/img/loading.gif" lazyload alt="Fig.3" width="720" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Fig.3: 扩散语言模型的分类体系，涵盖基础理论、训练与推理策略以及关键应用领域。章节编号(§)对应本综述中各章节位置。
    </span>
  </div>
</div>
<h2 id="2-Paradigms-of-Diffusion-Language-Models">2. Paradigms of Diffusion Language Models</h2>
<blockquote>
<p>扩散语言模型的范式</p>
</blockquote>
<p>DLMs 已成为一种强大的 <span style="color:lightgreen">非自回归范式</span>，能够平衡生成质量与推理并行性。受非平衡热力学原理[129]的启发，扩散语言模型通过学习逆转渐进式加噪过程实现生成。这种迭代优化方法支持对整个序列进行并行生成，为自回归 (AR) 模型的推理瓶颈提供了潜在解决方案。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510181723706.png" srcset="/img/loading.gif" lazyload alt="Table 1" width="720" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Table 1
    </span>
  </div>
</div>
<p>根据扩散过程所作用的空间，扩散语言模型大致可分为两类：<span style="color:lightgreen">连续空间型</span> 与 <span style="color:lightgreen">离散空间型</span>。此外，还存在 <span style="color:lightgreen">混合自回归-扩散模型</span> (hybrid AR-Diffusion models)，这类模型以多种形式融合自回归与扩散范式，旨在利用两种范式的互补优势。我们在 table 1 中呈现了多项研究中的模型信息，并在 Fig.4 中对不同范式进行了对比。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510192159200.png" srcset="/img/loading.gif" lazyload alt="Fig.4" width="500" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Fig.4: 跨范式扩散语言模型的训练与推理流程概览（含自回归模型作为对照）。自回归模型采用教师强制与因果注意力机制进行训练，而离散与连续两类扩散语言模型均使用完全双向注意力机制。以 BD3-LM[76] 为代表的块级扩散模型融合了自回归与扩散策略，其训练过程采用专门设计的块因果注意力掩码机制。
    </span>
  </div>
</div>
<h3 id="2-1-Preliminary-of-Modern-Language-Modeling">2.1 Preliminary of Modern Language Modeling</h3>
<blockquote>
<p>现代语言建模基础</p>
</blockquote>
<p>语言建模领域经历了多个不同范式的演变，每种范式都具有独特的架构选择、训练目标及相关的权衡取舍。在本小节中，我们将简要概述近年来大规模基于 Transformer 的语言建模范式，重点阐述其核心原理、数学公式表述及代表性模型。本文不涵盖早期方法，因为此处我们聚焦于现代大规模设计。</p>
<p>为理解扩散语言模型（DLMs）作为一种新颖且极具前景的替代方案（其可解决先前方法的关键局限性）的兴起奠定概念基础。</p>
<h4 id="2-1-1-Masked-Language-Models">2.1.1 Masked Language Models</h4>
<blockquote>
<p>掩码语言模型 MLMs</p>
</blockquote>
<p><span style="color:cyan">掩码语言模型</span> (MLMs) 因 BERT 模型[130] 而广泛普及，是一种基础语言建模范式——它采用基于 Transformer 的 encoder-only 架构，实现了预训练语言模型的规模扩展。这类模型概念上简洁但实证效果显著：<span style="color:lightgreen">通过预测输入序列中被随机掩码的 token，并同时利用该 token 的前文与后文语境，学习双向上下文表示。</span>其核心思路遵循 denoising autoencoder (DAE) 框架：<span style="color:lightgreen">先对部分输入 token 进行掩码处理，再训练模型恢复这些被掩码的 token</span>，数学表达式如下：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>M</mi><mi>L</mi><mi>M</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>x</mi><mo>∼</mo><mi mathvariant="script">D</mi></mrow></msub><msub><mi mathvariant="double-struck">E</mi><mrow><mi mathvariant="script">M</mi><mo>∼</mo><mtext>Mask</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></msub><mrow><mo fence="true">[</mo><mo>−</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>∈</mo><mi mathvariant="script">M</mi></mrow></munder><mi>log</mi><mo>⁡</mo><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>x</mi><mrow><mi mathvariant="normal">\</mi><mi mathvariant="script">M</mi></mrow></msub><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(1)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathcal{L}_{MLM}=\mathbb{E}_{x \sim \mathcal{D}} \mathbb{E}_{\mathcal{M} \sim \text{Mask}(x)}\left[-\sum_{i \in \mathcal{M}} \log P_{\theta}\left(x_{i} \mid x_{\backslash \mathcal{M}}\right)\right] \tag{1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0717em;vertical-align:-1.3217em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∼</span><span class="mord mathcal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathcal mtight">M</span><span class="mrel mtight">∼</span><span class="mord text mtight"><span class="mord mtight">Mask</span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">[</span></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8557em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight">M</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">\</span><span class="mord mathcal mtight">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">]</span></span></span></span><span class="tag"><span class="strut" style="height:3.0717em;vertical-align:-1.3217em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">1</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 表示输入序列，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">M</mi></mrow><annotation encoding="application/x-tex">\mathcal{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">M</span></span></span></span> 是被掩码位置的集合，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi mathvariant="normal">\</mi><mi mathvariant="script">M</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{\backslash \mathcal{M}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7858em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">\</span><span class="mord mathcal mtight">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span></span></span></span> 代表可见（未被掩码）的上下文。此外，BERT 还引入了 <span style="color:lightgreen">“下一句预测”</span> (Next Sentence Prediction, NSP) 目标，用于建模句间关系，其数学表达式为：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>N</mi><mi>S</mi><mi>P</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mo stretchy="false">(</mo><mi>A</mi><mo separator="true">,</mo><mi>B</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>∼</mo><mi mathvariant="script">D</mi></mrow></msub><mrow><mo fence="true">[</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><msub><mi>P</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo>∣</mo><mi>A</mi><mo separator="true">,</mo><mi>B</mi><mo stretchy="false">)</mo><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(2)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathcal{L}_{NSP}=\mathbb{E}_{(A, B, y) \sim \mathcal{D}}\left[-\log P_{\theta}(y \mid A, B)\right] \tag{2}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">NSP</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1052em;vertical-align:-0.3552em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">A</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span><span class="mrel mtight">∼</span><span class="mord mathcal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em;">]</span></span></span><span class="tag"><span class="strut" style="height:1.1052em;vertical-align:-0.3552em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">2</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>式中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>A</mi><mo separator="true">,</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(A, B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span></span> 是一对文本片段，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">y \in \{0, 1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">}</span></span></span></span> 用于指示片段 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span> 在原始文本中是否紧跟在片段 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span> 之后（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 表示是，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span> 表示否）。</p>
<p>BERT 在语言理解任务（如情感分析、命名实体识别、问答系统等）中的出色表现，催生出众多改进变体：</p>
<ul>
<li>RoBERTa[131]：移除了 NSP 目标，并采用更激进的训练策略（如更长的训练时长、更大的批次大小）；</li>
<li>ALBERT[132]：通过参数共享（如不同 Transformer 层间共享参数）与矩阵分解技术，提升模型训练与推理效率；</li>
<li>DeBERTa[133]：引入 <span style="color:lightgreen">“解耦注意力”</span> (disentangled attention) 机制增强上下文编码能力，并优化了掩码 token 预测的解码流程。</li>
</ul>
<p>尽管掩码语言模型在语言理解任务中优势显著，但<strong>其本质并非为生成任务设计</strong>：若要用于文本生成，需设计专门的微调策略或解码方案；若不进行大幅架构修改，这类模型无法适用于开放式文本生成任务。</p>
<h4 id="2-1-2-Autoregressive-Language-Models">2.1.2 Autoregressive Language Models</h4>
<blockquote>
<p>自回归语言模型</p>
</blockquote>
<p>以 GPT 系列[1]、[2]、[13]、[14] 和 Transformer-XL[134] 为代表，后续大语言模型（LLMs）[3]-[5]、[135]进一步推动其发展，<span style="color:cyan">自回归语言模型</span> 已成为现代生成式人工智能的核心支柱。其核心特征是 <span style="color:lightgreen">从左到右的单向 token 生成过程</span>。与双向模型不同，自回归语言模型将文本序列的联合概率分解为条件概率的乘积：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(3)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">P(x) = \prod_{i=1}^{n} P_{\theta}\left(x_{i} \mid x_{1}, x_{2}, \dots, x_{i-1}\right) \tag{3}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span><span class="tag"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">3</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>给定 token 序列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X = (x_1, x_2, \dots, x_n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，模型的训练目标是在上述概率分解框架下，最大化序列的对数似然：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>A</mi><mi>R</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>X</mi><mo>∼</mo><mi mathvariant="script">D</mi></mrow></msub><mrow><mo fence="true">[</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>log</mi><mo>⁡</mo><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>∣</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>−</mo><mn>1</mn></mrow></msub><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(4)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathcal{L}_{AR} = \mathbb{E}_{X \sim \mathcal{D}}\left[-\sum_{i=1}^{n} \log P_{\theta}\left(x_{i} \mid x_{1}, \dots, x_{i-1}\right)\right] \tag{4}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">A</span><span class="mord mathnormal mtight" style="margin-right:0.00773em;">R</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0277em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span><span class="mrel mtight">∼</span><span class="mord mathcal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">[</span></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">]</span></span></span></span><span class="tag"><span class="strut" style="height:3.0277em;vertical-align:-1.2777em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">4</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>该目标通常通过 <span style="color:lightgreen">decoder-only</span> Transformer 架构实现，训练过程中引入 <span style="color:lightgreen">因果注意力掩码</span>（causal attention masking）和 <span style="color:lightgreen">教师强制</span>（teacher forcing）机制：前者确保每个 token 的预测仅依赖于其前面的 token，后者则支持损失的并行计算。</p>
<p>自回归模型的“顺序生成”特性既是优势，也是局限：</p>
<ul>
<li><strong>优势</strong>：与文本生成任务的天然逻辑一致，采样过程直观，能自然适配问答、创作等各类应用场景；</li>
<li><strong>局限</strong>：对推理速度构成根本性瓶颈——token 生成过程本质是顺序执行的，无法并行化。</li>
</ul>
<p>这种生成质量与推理延迟的权衡，已成为自回归模型发展的核心挑战。除标准的 <span style="color:lightgreen">“下一个 token 预测”</span> (next-token prediction, NTP) 外，近年研究开始探索 <span style="color:lightgreen">“多token预测”</span> (multi-token prediction, MTP) [16]、[136]：通过每步生成多个 token 加速推理，其核心思路与扩散语言模型（DLMs）采用的并行解码策略存在概念上的相似性。</p>
<h4 id="2-1-3-Other-Paradigms">2.1.3 Other Paradigms</h4>
<h5 id="Sequence-to-Sequence-Models">Sequence-to-Sequence Models</h5>
<blockquote>
<p>序列到序列模型</p>
</blockquote>
<p><span style="color:cyan">序列到序列模型</span> (Seq2Seq) [137]是一种早期但仍具强大能力的范式，基于 <span style="color:lightgreen">encoder-decoder</span> 架构构建，是机器翻译、文本摘要等 <span style="color:lightgreen">条件文本生成任务</span> 的通用框架。T5[138]、BART[139]等现代模型是该范式的典型代表。</p>
<p>在该架构中，编码器首先处理源序列并生成中间表示，解码器再基于此中间表示生成目标序列——通常采用自回归方式。尽管标准 Seq2Seq 模型的解码器为自回归设计，但该框架本身具备高度灵活性：许多扩散语言模型（如 DiffuSeq[50]、SeqDiffuSeq[140]）对其进行适配，将自回归解码器替换为 <span style="color:lightgreen">非自回归扩散解码器</span>，借助编码器强大的条件建模能力，引导生成过程中的去噪操作。</p>
<h5 id="Permutation-Language-Models">Permutation Language Models</h5>
<blockquote>
<p>排列语言模型</p>
</blockquote>
<p><span style="color:cyan">排列语言模型</span> (Permutation Language Models, PLMs) 以 XLNet[141] 为代表，为“在生成式框架中融入双向上下文”提供了另一种思路。这类模型的训练目标仍是预测序列中的 token，但并非采用固定的从左到右顺序，而是通过 <span style="color:lightgreen">随机排列的顺序</span> 进行预测。其训练目标是在所有可能的概率分解顺序排列下，最大化期望对数似然：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>P</mi><mi>L</mi><mi>M</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>z</mi><mo>∼</mo><msub><mi mathvariant="script">Z</mi><mi>T</mi></msub></mrow></msub><mrow><mo fence="true">[</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><msub><mi>P</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msub><mi>x</mi><msub><mi>z</mi><mi>t</mi></msub></msub><mo>∣</mo><msub><mi>x</mi><msub><mi>z</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub></msub><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(5)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathcal{L}_{PLM} = \mathbb{E}_{z \sim \mathcal{Z}_{T}}\left[-\sum_{t=1}^{N} \log P_{\theta}\left(x_{z_{t}} \mid x_{z_{&lt;t}}\right)\right] \tag{5}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mord mathnormal mtight">L</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:0.07944em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3567em;margin-left:-0.0794em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1433em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2503em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">[</span></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1709em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2697em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">]</span></span></span></span><span class="tag"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">5</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="script">Z</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">\mathcal{Z}_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.07944em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0794em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示长度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> 的序列的所有可能排列的集合；<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">z_{&lt;t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6079em;vertical-align:-0.1774em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span></span></span></span> 分别表示给定排列 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>∈</mo><msub><mi mathvariant="script">Z</mi><mi>T</mi></msub></mrow><annotation encoding="application/x-tex">z \in \mathcal{Z}_T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.07944em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0794em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 中的第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 个元素和前 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6984em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 个元素。这种设计使模型能为每个 token 捕捉双向上下文，既融合了 MLMs “双向上下文”的优势，又保留了自回归模型“连贯生成过程”的特点。这与 DLMs 形成鲜明对比——DLMs 通过“并行迭代优化过程”实现双向上下文建模。</p>
<h3 id="2-2-Continuous-Diffusion-Language-Models">2.2 Continuous Diffusion Language Models</h3>
<blockquote>
<p>连续空间扩散语言模型</p>
</blockquote>
<p><span style="color:cyan">连续空间扩散语言模型</span> (Continuous-space DLMs) 对语言建模的 <span style="color:red">核心思路</span> 是：<span style="color:lightgreen">先将离散 token 映射到连续嵌入空间，再在该空间中通过扩散过程建模数据分布</span>[22]、[23]。通常，<span style="color:cyan">扩散模型</span>通过学习逆转预定义的加噪过程，来定义生成过程——该加噪过程会将数据逐步转化为噪声。整个过程包含 <strong>前向（加噪）过程</strong> 与 <strong>反向（去噪）过程</strong> 两部分：</p>
<h4 id="2-2-1-Forward-noising-Process">2.2.1 Forward (noising) Process</h4>
<p>前向过程通过固定的马尔可夫链，在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span> 个时间步内将数据样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 逐步转化为噪声，其概率公式可表示为：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>∣</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∏</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(6)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">q(x_{1:T} \mid x_0) = \prod_{t=1}^{T} q(x_t \mid x_{t-1}) \tag{6}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mrel mtight">:</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">6</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中，每个时间步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 的状态转移概率服从正态分布：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="script">N</mi><mrow><mo fence="true">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">;</mo><msub><mi>μ</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi mathvariant="normal">Σ</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(7)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">q(x_t \mid x_{t-1}) = \mathcal{N}\left(x_t; \mu_t(x_{t-1}), \Sigma_t\right) \tag{7}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;">)</span></span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">7</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>式中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>μ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mu_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（均值）和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">Σ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\Sigma_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（协方差矩阵）共同定义了 <strong>噪声调度</strong>（noise schedule）。在许多实际实现中（如DDPM[18]、Rectified Flow[21]），每个时间步的边缘分布可表示为闭式解：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><msub><mi>α</mi><mi>t</mi></msub><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mi>t</mi></msub><mi>ϵ</mi><mo separator="true">,</mo><mspace width="1em"/><mi>ϵ</mi><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(8)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">x_t = \alpha_t x_0 + b_t \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) \tag{8}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">ϵ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">8</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>这里，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">b_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是关于时间步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 的确定性函数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 是服从标准正态分布（均值为0、协方差矩阵为单位矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi></mrow><annotation encoding="application/x-tex">I</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span></span></span></span>）的噪声。</p>
<h4 id="2-2-2-Backward-denoising-Process-Objective">2.2.2 Backward (denoising) Process &amp; Objective</h4>
<p>反向过程的核心是“逆转加噪过程”：从服从标准正态分布的噪声 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>T</mi></msub><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_T \sim \mathcal{N}(0, I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span> 出发，通过逐步去噪恢复出与原始样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 接近的结果。该过程由神经网络 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{\theta}(x_t, t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span> 参数化实现（通常采用 Transformer 架构），其核心任务是预测与前向过程相关的目标量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span>（如干净数据、噪声或速度）。</p>
<p>连续空间 DLMs 的常用训练目标函数形式如下：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>s</mi><mi>i</mi><mi>m</mi><mi>p</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>t</mi><mo separator="true">,</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><mi>z</mi></mrow></msub><mrow><mo fence="true">[</mo><msup><mrow><mo fence="true">∥</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo>−</mo><mi>z</mi><mo fence="true">∥</mo></mrow><mn>2</mn></msup><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(9)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathcal{L}_{simple} = \mathbb{E}_{t, x_0, z}\left[\left\|f_{\theta}(x_t, t) - z\right\|^2\right] \tag{9}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">im</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">pl</span><span class="mord mathnormal mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">[</span></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em;">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose delimcenter" style="top:0em;">∥</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954em;"><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">]</span></span></span></span><span class="tag"><span class="strut" style="height:1.8em;vertical-align:-0.65em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">9</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是由原始样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 通过前向过程采样得到的带噪样本，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span> 是由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和时间步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 推导得到的对应回归目标。</p>
<h4 id="2-2-3-Generation-Process">2.2.3 Generation Process</h4>
<p>模型训练完成后，生成过程通过“从学习到的反向过程中采样”实现，具体步骤如下：</p>
<ol>
<li><strong>初始化</strong>：从标准正态分布中采样初始噪声 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>T</mi></msub><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_T \sim \mathcal{N}(0, I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathcal" style="margin-right:0.14736em;">N</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mclose">)</span></span></span></span>；</li>
<li><strong>迭代去噪</strong>：在时间步 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi><mo separator="true">,</mo><mi>T</mi><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t = T, T-1, \dots, 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span></span></span></span> 上，模型定义条件分布 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∣</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{\theta}(x_{t-1} \mid x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，用于近似真实的反向转移概率 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∣</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_{t-1} \mid x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>；通过从这些学习到的条件分布中迭代采样，latent 状态的噪声会逐步降低，直至恢复出原始数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的估计值；</li>
<li><strong>离散化映射</strong>：生成去噪后的嵌入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\hat{x}_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 后，需通过 rounding step 将其映射回离散 token —— 通常通过 <span style="color:lightgreen">“嵌入空间中的最近邻搜索”</span> 或 <span style="color:lightgreen">“解码器头”</span> (decoder head) 实现。</li>
</ol>
<h4 id="2-2-4-代表性连续空间-DLM-模型">2.2.4 代表性连续空间 DLM 模型</h4>
<ol>
<li>Diffusion-LM[22]：首次在嵌入空间中引入扩散过程，构建了非自回归语言生成模型。通过借鉴图像扩散模型中的 <span style="color:lightgreen">分类器引导机制</span>，实现了高度可控的文本生成与文本填充 (text infilling)。</li>
<li>LDEBM[60]：在变分学习框架下，提出 latent 空间能量模型 (Energy-Based Models, EBMs) 与扩散模型的新型协同机制，解决了基于能量先验的学习难题，重点优化了文本建模的可解释性。</li>
<li>LATENTOPS[49]：在紧凑 latent 空间中实现可组合文本操作的高效框架。引入基于 ODE 的高效采样器，在 <span style="color:lightgreen">即插即用控制算子</span> 的引导下生成 latent 向量，再解码为目标文本。</li>
<li>Diffuseq[50]：面向序列到序列（Seq2Seq）任务的无分类器引导扩散语言模型。前向过程中仅对目标序列嵌入进行加噪，实现了性能强劲且多样性丰富的条件文本生成。</li>
<li>SED[23]：(Self-conditioned Embedding Diffusion) 直接在固定的连续 token 嵌入空间中进行扩散。通过引入 <span style="color:lightgreen">自条件机制</span>，其条件与无条件文本生成性能均表现出色，可与标准自回归模型抗衡。</li>
<li>CDCD[51]：通过将 token 嵌入到连续空间，实现对类别数据的连续扩散建模。提出“分数插值”技术（支持用交叉熵损失训练模型）与“时间扭曲”策略（训练中高效调度噪声强度的自适应方法）。</li>
<li>Difformer[52]：针对嵌入空间的优化难题，引入 <span style="color:lightgreen">锚定损失</span> (anchor loss) 防止嵌入坍缩，并设计 <span style="color:lightgreen">噪声重缩放框架</span> (noise rescaling framework) 缓解模型退化，提升生成稳定性。</li>
<li>LD4LG[53]：<span style="color:lightgreen">将预训练语言模型作为强大的自编码器</span>，构建紧凑 latent 空间；在该空间中训练连续扩散模型，实现高质量文本生成。</li>
<li>GENIE[54]：面向扩散语言模型的大规模预训练框架 (large-scale pre-training framework)，提出 <span style="color:lightgreen">连续段落去噪目标</span> (continuous paragraph denoise objective)，通过重构受损文本段落，实现从大规模语料中的高效学习。</li>
<li>InfoDiffusion[55]：引入 <span style="color:lightgreen">信息熵感知噪声调度</span>，引导模型遵循 <span style="color:lightgreen">核心信息优先</span> (KeyInfo-first) 的类人生成逻辑，优先生成文本的核心内容。</li>
<li>EDDPMs[56]：通过参数化 encoder-decoder 泛化扩散过程，统一了生成 (generation)、重构 (reconstruction) 与表示 (representation) 三大任务，支持在单一框架内对所有组件进行稳定联合训练。</li>
<li>SMOOTHIE[57]：提出 <span style="color:lightgreen">基于语义相似性逐步平滑 token 嵌入的新型扩散过程</span>，融合了连续 latent 空间的灵活性与离散 token 处理的准确性。</li>
</ol>
<blockquote>
<p>基于 logit 空间的连续扩散扩展<br>
连续扩散过程也可在 <span style="color:lightgreen">logit 空间</span>（而非嵌入空间）中构建：</p>
</blockquote>
<ul>
<li>TESS[58]：提出完全非自回归框架，在 token 的 <span style="color:lightgreen">k-logit 单纯形表示</span> (k-logit simplex representation) 上进行扩散，并设计适配该场景的新型自条件机制。</li>
<li>TESS 2[59]：进一步扩展 TESS 的规模——通过扩散专用预训练流程与指令微调，<span style="color:lightgreen">将预训练大型自回归模型适配为通用扩散语言模型</span>，赋予模型强大的指令跟随能力。</li>
</ul>
<h3 id="2-3-Discrete-Diffusion-Language-Models">2.3 Discrete Diffusion Language Models</h3>
<blockquote>
<p>离散空间扩散语言模型</p>
</blockquote>
<p><span style="color:cyan">离散空间扩散语言模型</span> (Discrete-space DLMs) 直接在 token 词汇表上定义扩散过程，无需在扩散过程本身中使用连续嵌入空间。</p>
<p>D3PM[24] 首次通过在离散 token 上引入结构化扩散过程，验证了该思路的可行性。其前向过程通过在 <span style="color:lightgreen">每一步应用转移矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">Q_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 对序列进行加噪，该矩阵定义了一个 token 转移到词汇表中任意其他 token 的概率</span>。给定初始状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 时，状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的概率服从类别分布 (Categorical Distribution)：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo>∣</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>=</mo><mtext>Cat</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">;</mo><mi>p</mi><mo>=</mo><msub><mi>x</mi><mn>0</mn></msub><msub><mover accent="true"><mi>Q</mi><mo stretchy="true">‾</mo></mover><mi>t</mi></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mspace width="1em"/><mtext>where </mtext><msub><mover accent="true"><mi>Q</mi><mo stretchy="true">‾</mo></mover><mi>t</mi></msub><mo>=</mo><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msub><mi>Q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">q(x_t \mid x_0) = \text{Cat}(x_t; p = x_0 \overline{Q}_t), \quad \text{where} \ \overline{Q}_t = \prod_{i=1}^{t} Q_i
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Cat</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1333em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8833em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span></span></span><span style="top:-3.8033em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em;"></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord text"><span class="mord">where</span></span><span class="mspace"> </span><span class="mord"><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8833em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span></span></span><span style="top:-3.8033em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0582em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7806em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">Q_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的常见选择是 <span style="color:lightgreen">吸收态转移</span> (absorbing state transition)：每个 token 要么以一定概率保持不变，要么转移到特殊的 “[MASK]” token。反向过程则学习逆转这些转移——基于受损序列预测原始 token 的概率分布。</p>
<p>随着研究发展，<span style="color:cyan">掩码类扩散语言模型</span> (masked DLMs) 已成为离散扩散语言模型的一种现代且高效的演进形式，为近年来多项大规模研究[27]、[28]奠定了基础。以该类模型中最具代表性的 LLaDA[28] 为例：受早期“重参数化与简化训练目标”相关研究[61]、[62]、[68]的启发，LLaDA 采用从零开始训练的方式，损失函数为仅对被掩码 token 计算的交叉熵损失，公式如下：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><mi mathvariant="script">L</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>≜</mo><mo>−</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>t</mi><mo separator="true">,</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub></mrow></msub><mrow><mo fence="true">[</mo><mfrac><mn>1</mn><mi>t</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mn mathvariant="double-struck">1</mn><mo stretchy="false">[</mo><msubsup><mi>x</mi><mi>t</mi><mi>i</mi></msubsup><mo>=</mo><mi>M</mi><mo stretchy="false">]</mo><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msubsup><mi>x</mi><mn>0</mn><mi>i</mi></msubsup><mo>∣</mo><msub><mi>x</mi><mi>t</mi></msub><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(10)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathcal{L}(\theta) \triangleq -\mathbb{E}_{t, x_0, x_t}\left[\frac{1}{t} \sum_{i=1}^{L} \mathbb{1}[x_t^i = M] \log p_{\theta}\left(x_0^i \mid x_t\right)\right] \tag{10}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1667em;vertical-align:-0.25em;"></span><span class="mord mathcal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel amsrm">≜</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord">−</span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mpunct mtight">,</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">[</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8747em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8747em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">]</span></span></span></span><span class="tag"><span class="strut" style="height:3.106em;vertical-align:-1.2777em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">10</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 从训练语料中采样，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 从 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 上均匀采样，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 通过前向过程加噪后得到的序列；指示函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn mathvariant="double-struck">1</mn><mo stretchy="false">[</mo><mo>⋅</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\mathbb{1}[\cdot]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mopen">[</span><span class="mord">⋅</span><span class="mclose">]</span></span></span></span> 确保损失仅作用于已被掩码的位置（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>t</mi><mi>i</mi></msubsup><mo>=</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">x_t^i = M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0717em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8247em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> 表示 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 个位置为掩码符号 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>，即“[MASK]”）。</p>
<p>在推理阶段，生成过程从 <span style="color:lightgreen">期望长度的全掩码序列</span> 开始，具体步骤如下：</p>
<ol>
<li>迭代去噪：每一步中，模型接收当前序列（包含已生成 token 与 “[MASK]”token 的混合），并预测完整的 token 序列；</li>
<li>动态掩码调整：根据模型的预测置信度与噪声调度，将一定数量置信度最高的预测结果解掩码并固定，剩余位置则重新掩码；</li>
<li>终止条件：该优化过程迭代进行，直至所有 “[MASK]”token 都被解析。</li>
</ol>
<p>这种方法巧妙结合了 <span style="color:cyan">MLMs</span> 的双向上下文建模能力与可控的并行生成过程。其中，LLaDA-8B 模型尤为突出——不仅展现出强大的可扩展性与指令跟随能力，性能还能与 LLaMA3-8B 等高性能自回归模型相当，对自回归模型在大规模语言生成领域的长期主导地位构成了挑战。</p>
<p>DiffusionBERT[25] 将预训练 BERT 与离散扩散过程相结合：借助 BERT 强大的去噪能力，从掩码状态中学习反向过程。此外，该模型还通过一种新颖的 <span style="color:lightgreen">纺锤噪声</span> (spindle noise) 调度策略进一步提升性能——该策略会考虑 token 的信息重要性，使生成质量较此前的扩散语言模型有显著提升。</p>
<p>另一种方法是 <span style="color:lightgreen">重参数化</span> 离散扩散模型 (Reparameterized Discrete diffusion Models, RDM) [61]，它为逆向过程建立了一种替代公式，将训练目标简化为加权交叉熵损失。这使得解码策略更具灵活性和适应性，相比以往的离散扩散模型，性能获得了显著提升。类似地，MD4[62]将交叉熵损失的加权积分作为掩码扩散模型的连续时间变分目标，为扩散语言模型的训练提供了一个简洁且通用的框架。</p>
<p>另一种类似的方法是 MDLM[63]，它引入了一种简化的 Rao-Blackwell 化目标，该目标以掩码语言建模损失的加权平均值形式呈现。Diffusion-LLM[64] 通过将预训练掩码语言模型适配到扩散范式，并进一步进行任务特定微调与指令微调，展现了扩散语言模型的可扩展性，从而解锁了其在解决通用语言任务中的通用性。Diffusion-NAT[65]通过将去噪过程重构为非自回归掩码令牌恢复任务，将 Discrete DLMs 与 PLMs 相融合，使 BART 能够作为有效的去噪器发挥作用。</p>
<p>Plaid[66] 是首个以最大化数据似然为训练目标的扩散语言模型，其通过 scaling laws 证明，在标准基准测试中，该模型的性能可优于 GPT-2 等自回归模型。为改进训练目标，SEDD[67] 引入了 <span style="color:lightgreen">得分熵损失</span> (score entropy loss)，用于直接学习数据分布的比率，这是得分匹配 (score matching) 的离散扩展。RADD[68] (Reparameterized Absorbing Discrete Diffusion, 重参数化吸收离散扩散) 发现，吸收扩散中的具体得分可表示为干净数据的与时间无关的条件概率，再乘以一个与时间相关的解析标量。此外，该方法还正式统一了吸收离散扩散与任意阶自回归模型的训练目标。</p>
<p>DFM[69] (Discrete Flow Matching, 离散流匹配) 为离散数据引入了一种新颖的生成范式，该范式与连续流匹配 (continuous Flow Matching) 类似。该方法通过学习生成概率速度，使样本能够沿着从源分布到目标分布的通用概率路径进行转换。通过缩放模型架构，DFM 在各类基准测试中显著缩小了与自回归模型的性能差距。DDPD[70] 提出了一个将生成过程解耦为两个专用模型的框架：<span style="color:lightgreen">规划器</span> (planner) 和 <span style="color:lightgreen">去噪器</span> (denoiser)。在每一步中，规划器识别出最需要优化的受损令牌位置，随后由去噪器预测这些位置的令牌值。</p>
<p>为提升模型在复杂推理任务中的性能，研究人员提出了 MGDM[71] 以解决子目标失衡问题。该方法通过令牌级重加权机制，在学习过程中优先处理更复杂的子目标，从而增强了离散扩散的效果。为应对模型缩放挑战，有研究[27]提出了一种持续预训练方法，将 LLaMA 等现有的自回归模型适配为扩散语言模型。由此得到的模型（命名为 DiffuGPT 和 DiffuLLaMA）不仅能与 AR 模型媲美，还具备了扩散模型特有的能力（如灵活填充）。</p>
<p>基于这一思路，Dream-7B[26] 以 Qwen2.5 7B[142] 为初始化模型，并用 580 B 个令牌进行进一步训练，其性能大幅优于现有扩散语言模型，且能比肩顶级自回归模型。GIDD[72] 的提出旨在克服掩码扩散模型无法修正已生成令牌的局限：该框架通过 <span style="color:lightgreen">将掩码与均匀噪声结合，对加噪过程进行泛化</span>，从而使模型具备了自我修正错误的能力，并提升了样本质量。</p>
<p>近期，为提升模型的长上下文能力，LongLLaDA[73] 首次对扩散语言模型在该领域的表现进行了系统性分析。研究发现，扩散语言模型在直接上下文外推过程中能保持稳定的困惑度，且具有更优的检索能力。此外，LongLLaDA 还引入了一种基于 NTK 的无训练 RoPE 外推方法，该方法显著提升了外推性。</p>
<h3 id="2-4-Hybrid-AR-Diffusion-Language-Models">2.4 Hybrid AR-Diffusion Language Models</h3>
<blockquote>
<p>混合自回归-扩散语言模型</p>
</blockquote>
<p><span style="color:cyan">混合自回归-扩散模型</span> (Hybrid AR-Diffusion Models) 旨在在非自回归模型的全并行性与自回归模型强大的因果依赖建模能力之间取得平衡。混合自回归-扩散建模的核心策略之一是采用 <span style="color:lightgreen">块级半自回归生成过程</span>：在该框架下，模型 <span style="color:lightgreen">以自回归方式生成令牌块</span> (blocks of tokens)，而每个块内部的令牌则通过类扩散的迭代过程并行生成。</p>
<p>早期研究如 SSD-LM[74] 开创了混合方法的先河，其在单纯形表示 (simplex representations) 上通过块级连续扩散过程实现建模；AR-DIFFUSION[75] 则提出了多级扩散过程，并通过根据令牌位置调整时间步长 (timestep) 实现半自回归生成。近期的代表性模型 BD3-LM[76] 在离散模型上进一步推进了这一方向，其性能显著优于纯自回归模型和纯扩散模型。CtrlDiff[77] 则通过引入动态块预测技术改进了该范式，提升了块级生成的效率与可控性。</p>
<p>这类模型的生成过程通常包含两个嵌套循环：</p>
<ul>
<li><strong>外循环</strong>：以自回归方式生成令牌块，每个块的生成均以先前生成的块为条件；</li>
<li><strong>内循环</strong>：在每个块内部，通过类扩散的迭代去噪过程实现令牌级并行生成。</li>
</ul>
<p>在 BD3-LM 中，训练目标被形式化为：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi mathvariant="script">L</mi><mrow><mi>B</mi><mi>D</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><mi>B</mi></munderover><msub><mi mathvariant="double-struck">E</mi><mrow><mi>t</mi><mo>∼</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></msub><msub><mi mathvariant="double-struck">E</mi><mi>q</mi></msub><mfrac><mn>1</mn><mi>t</mi></mfrac><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msup><mi>x</mi><mi>b</mi></msup><mi mathvariant="normal">∣</mi><msubsup><mi>x</mi><mi>t</mi><mi>b</mi></msubsup><mo separator="true">,</mo><msup><mi>x</mi><mrow><mo>&lt;</mo><mi>b</mi></mrow></msup><mo fence="true">)</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(11)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">\mathcal{L}_{BD}(x, \theta) := -\sum_{b=1}^{B} \mathbb{E}_{t \sim [0,1]} \mathbb{E}_{q} \frac{1}{t} \log p_{\theta}\left(x^{b} | x_{t}^{b}, x^{&lt;b}\right) \tag{11}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathcal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">:=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1304em;vertical-align:-1.3021em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mrel mtight">∼</span><span class="mopen mtight">[</span><span class="mord mtight">0</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span><span class="mclose mtight">]</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-2.453em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span><span class="tag"><span class="strut" style="height:3.1304em;vertical-align:-1.3021em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">11</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>该混合策略使模型能够通过自回归捕捉块间的长程依赖，同时通过并行扩散加速块内令牌的生成。此外，该设计还支持灵活的输出长度，并兼容自回归模型中广泛使用的 <span style="color:lightgreen">键值缓存</span> (KV-Cache) [76]。</p>
<p>值得注意的是，近期的掩码扩散语言模型[28]、[31]也采用了类似的半自回归块级解码策略，这类模型可被视为混合自回归-扩散建模的典型实例。</p>
<p>除了在序列层面结合自回归与扩散的块级方法外，混合还可发生在<strong>架构层面</strong>：神经网络的某一部分（通常是编码器）将整个序列扩散为中间表示，随后由自回归解码器生成最终序列[143]。LADIDA[144] 采用了略有不同的思路，其在文档层面进行扩散，但通过自回归解码器对句子进行解码；SpecDiff[78] 则提出了一种协同推测解码框架（collaborative speculative decoding framework）：由轻量级扩散模型生成候选输出，再由大型自回归模型对候选输出进行验证与最终确定。</p>
<h2 id="3-DLMs-Pre-training-and-Post-training">3 DLMs: Pre-training and Post-training</h2>
<blockquote>
<p>扩散语言模型：预训练与后训练</p>
</blockquote>
<h3 id="3-1-Pre-training-and-Supervised-Fine-tuning">3.1 Pre-training and Supervised Fine-tuning</h3>
<blockquote>
<p>预训练与监督微调</p>
</blockquote>
<p>扩散语言模型 (DLMs) 的预训练流程在很大程度上遵循与自回归语言模型（适用于离散扩散语言模型）或图像扩散模型（适用于连续扩散语言模型）相似的步骤，但其设计空间相对较少。本节简要总结现有扩散语言模型的预训练方法，旨在弥合 DLMs 与 ARMs 之间的方法学差距。</p>
<p>为加快训练速度（尤其是针对大规模模型），常见做法是 <span style="color:lightgreen">从预训练的自回归语言模型或图像扩散模型</span> 中初始化 DLMs。DiffuGPT与DiffuLLaMA[27] 尝试使用参数规模从 127M ~ 7B 的开源 LLMs 初始化掩码扩散语言模型，结果表明：<span style="color:lightgreen"> 扩散语言模型可从自回归模型中高效适配而来，在显著缩短训练时间、降低训练成本的同时，还能实现与自回归同类模型相当甚至更优的性能。</span> 基于这一发现，Dream-7B 以 Qwen 2.5 7B[142] 为初始模型，据报道其在各类基准测试中的性能均优于 LLaDA-8B与LLaMA3-8B。</p>
<p>另一方面，部分多模态扩散语言模型（multimodal DLMs）从预训练的图像扩散模型中初始化。例如，D-DiT[79] 与 Muddit[80] 分别从 SD3[11] 和 Meissonic[145] 的预训练 MM-DiT 骨干网络中初始化。尽管这些模型最初并非为文本生成设计，但其 latent 表示中包含内在的与语言对齐的知识，这既能有效促进语言建模的训练，又能保留强大的视觉生成能力。</p>
<p>扩散语言模型的 <span style="color:lightgreen">监督微调</span> (SFT) 流程通常与自回归模型相似。对于 LLaDA[28]这类掩码扩散语言模型，<span style="color:lightgreen">提示令牌</span> (prompt tokens) 保持不掩码状态，而 <span style="color:lightgreen">响应令牌</span> (response tokens) 则被选择性掩码，这种方式能让模型以与预训练兼容的模式学习条件响应生成。在连续扩散语言模型中，监督微调还可通过仅对响应片段进行加噪实现，如 TESS2[59] 所示。</p>
<p>尽管扩散语言模型的训练流程与自回归模型整体相似，但由于其基于扩散的架构设计，仍面临一些独特挑战。其中一个 <span style="color:red">主要问题</span> 是 <span style="color:lightgreen">掩码扩散语言模型的损失计算效率较低</span>：在典型的掩码扩散语言模型训练中，若对时间步(timestep) 进行均匀采样，平均仅 50% 的令牌会参与损失计算。这会降低数据利用率，并可能导致模型获得非最优梯度——尤其是当关键答案令牌未参与损失计算时，该问题更为突出。</p>
<p>为解决这一问题，LaViDa[96]提出了一种互补掩码策略：将每个训练样本复制为两个带有 <span style="color:lightgreen">不重叠掩码模式</span> 的样本，确保所有令牌至少参与一次损失计算。此外，如文献[146]所示，由于存在 <span style="color:lightgreen">训练-推理差异</span> (train-inference discrepancy)，模型在训练阶段的性能显著优于推理阶段。对此，作者提出了一种两步扩散流程与改进的调度技术，以缓解这一问题。</p>
<h3 id="3-2-Post-training-for-Reasoning-Capabilities">3.2 Post-training for Reasoning Capabilities</h3>
<blockquote>
<p>面向推理能力的后训练</p>
</blockquote>
<p>随着 DLMs 在语言任务上的性能不断提升，其推理能力的探索也日益受到关注。通常情况下，模型的推理能力需通过在推理数据集上进行微调来获得。但这一过程对扩散语言模型而言，面临着独特且艰巨的挑战：传统的思维链 (Chain-of-Thought, CoT) 方法依赖 AR 模型的 <span style="color:lightgreen">序列生成特性</span> 实现逐步推理，而扩散语言模型采用并行方式生成令牌；在自回归模型领域中最成功的后训练技术（尤其是基于 RL 和策略梯度的方法），其核心是 <span style="color:lightgreen">能够高效计算生成序列的对数概率</span>——得益于自回归模型的可分解性与序列特性，这一计算在自回归模型中十分直接。但扩散语言模型的生成过程是迭代式、非序列的，其对数似然难以求解，这为将成熟的自回归模型强化学习算法套件应用于扩散语言模型设置了重大技术障碍。</p>
<p>基于直观分类，我们将现有针对扩散语言模型推理能力的后训练工作划分为三大方向，构成本节的核心内容：</p>
<ol>
<li>推理链并行化，即把自回归模型中的 CoT 适配到扩散语言模型的并行生成过程中；</li>
<li>策略梯度方法适配，即向扩散语言模型引入 <span style="color:lightgreen">GRPO</span> 等主流算法的变体；</li>
<li>偏好优化方法适配，即向扩散语言模型引入 <span style="color:lightgreen">直接偏好优化 (DPO)</span> 等方法。</li>
</ol>
<h4 id="3-2-1-DoT-and-DCoLT-Parallelizing-the-Reasoning-Chain">3.2.1 DoT and DCoLT: Parallelizing the Reasoning Chain</h4>
<blockquote>
<p>DoT 与 DCoLT：推理链并行化</p>
</blockquote>
<p>在扩散语言模型中实现复杂推理的开创性工作之一是 <span style="color:lightgreen">思维扩散</span> (Diffusion-of-Thought, DoT)[81]，该方法将主流的思维链范式适配到扩散框架中。与自回归模型按序列生成推理步骤不同，DoT 将推理步骤构建为“中间思维”，并在整个扩散去噪过程中对这些中间思维进行并行优化。具体实现方式为：在包含问题及其对应逐步推理过程的数据集上，对 Plaid[66]、SEDD[67] 等预训练扩散语言模型进行微调。</p>
<p>为提升模型的自我纠错能力，DoT 引入了 <span style="color:lightgreen">调度采样</span> (scheduled sampling) 和 <span style="color:lightgreen">耦合采样</span> (coupled sampling) 等专用训练技术——在训练过程中让模型接触自身生成的错误，从而增强其自我修正能力。这种后训练方法使小型扩散语言模型能实现出色的推理性能，甚至在部分数学和逻辑推理基准测试中，表现优于规模大得多的自回归模型。</p>
<p>近期提出的 <span style="color:lightgreen">横向思维扩散链</span> (Diffusion Chain of Lateral Thought, DCoLT) [82] 则引入了一种独特的基于强化学习的推理框架，其灵感来源于“横向思维”这一认知概念，与传统思维链方法的“逐步纵向思维”形成鲜明对比。DCoLT 不对中间推理步骤进行监督，而是将逆向扩散过程的每一步视为“潜在思维动作”，并通过基于结果的强化学习优化整个多步去噪轨迹，以最大化最终答案的奖励值。</p>
<p>在将 DCoLT 应用于 LLaDA 等掩码扩散语言模型时，该方法创新性地引入了 <span style="color:lightgreen">解掩码策略模块</span> (Unmasking Policy Module, UPM)——该模块将学习令牌的最优揭示顺序作为强化学习动作空间的一部分。这种方法显著提升了扩散语言模型的推理能力：经 DCoLT 强化的 LLaDA 模型在 GSM8K 基准测试中性能提升 9.8%，在 HumanEval 基准测试中性能提升 19.5%。</p>
<h4 id="3-2-2-Adapting-Policy-Gradient-Methods-to-DLMs">3.2.2 Adapting Policy Gradient Methods to DLMs</h4>
<blockquote>
<p>策略梯度方法向扩散语言模型的适配</p>
</blockquote>
<p><span style="color:lightgreen">得分熵策略优化</span> (Score Entropy Policy Optimization, SEPO) [83] 首次将 <span style="color:lightgreen">基于人类反馈的强化学习</span> (RLHF) 引入离散扩散语言模型，提出了一个具有理论支撑的框架——<span style="color:lightgreen">利用策略梯度方法和非可微奖励对离散扩散模型进行微调。</span></p>
<p>SEPO 在得分熵框架下运行，通过重要性采样推导稳定且低方差的梯度估计，从而适配 PPO、GRPO 等现代策略梯度方法。这使得模型的策略可通过迭代更新最大化奖励函数，进而成为适用于条件生成与无条件生成的通用框架。SEPO 的目标函数定义如下：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msup><mi>l</mi><mi>A</mi></msup><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mrow><mi>x</mi><mo>∼</mo><msub><mi>π</mi><msub><mi>θ</mi><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></mrow></msub><mrow><mo fence="true">[</mo><munder><mo>∑</mo><mstyle scriptlevel="1"><mtable rowspacing="0.1em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>y</mi><mo>∈</mo><mi mathvariant="script">X</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="1" displaystyle="false"><mrow><mi>y</mi><mo mathvariant="normal">≠</mo><mi>x</mi></mrow></mstyle></mtd></mtr></mtable></mstyle></munder><msub><mi>w</mi><mrow><mi>x</mi><mo separator="true">,</mo><mi>y</mi></mrow></msub><mi>log</mi><mo>⁡</mo><msub><mi>s</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>T</mi><mo>−</mo><msub><mi>T</mi><mn>0</mn></msub><msub><mo stretchy="false">)</mo><mi>y</mi></msub><mo fence="true">]</mo></mrow></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(12)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">l^A(\theta) = \mathbb{E}_{x \sim \pi_{\theta_{old}}}\left[
\sum_{\substack{y \in \mathcal{X} \\ y \neq x}} w_{x,y} \log s_{\theta}(x, T-T_0)_y
\right] \tag{12}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:4.4026em;vertical-align:-2.0527em;"></span><span class="mord"><span class="mord mathbb">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∼</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0278em;margin-right:0.1em;"><span class="pstrut" style="height:2.6944em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3496em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.401em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4307em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35em;"><span style="top:-4.35em;"><span class="pstrut" style="height:6.2em;"></span><span style="width:0.667em;height:4.200em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="4.200em" viewBox="0 0 667 4200"><path d="M403 1759 V84 H666 V0 H319 V1759 v600 v1759 h347 v-84
H403z M403 1759 V0 H319 V1759 v600 v1759 h84z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.85em;"><span></span></span></span></span></span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.5407em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1333em;"><span style="top:-3.15em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∈</span><span class="mord mathcal mtight" style="margin-right:0.14643em;">X</span></span></span><span style="top:-2.2611em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight"><span class="mrel mtight"><span class="mord vbox mtight"><span class="thinbox mtight"><span class="rlap mtight"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord mtight"><span class="mrel mtight"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel mtight">=</span></span><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.6333em;"><span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.0527em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.35em;"><span style="top:-4.35em;"><span class="pstrut" style="height:6.2em;"></span><span style="width:0.667em;height:4.200em;"><svg xmlns="http://www.w3.org/2000/svg" width="0.667em" height="4.200em" viewBox="0 0 667 4200"><path d="M347 1759 V0 H0 V84 H263 V1759 v600 v1759 H0 v84 H347z
M347 1759 V0 H263 V1759 v600 v1759 h84z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.85em;"><span></span></span></span></span></span></span></span></span><span class="tag"><span class="strut" style="height:4.4026em;vertical-align:-2.0527em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">12</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中，模型参数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span> 的优化目标是最大化得分熵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">s_{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的加权期望对数似然，权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>x</mi><mo separator="true">,</mo><mi>y</mi></mrow></msub><mo>=</mo><msub><mi>π</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mi>f</mi><mo stretchy="false">(</mo><msubsup><mi>r</mi><mrow><mi>x</mi><mo separator="true">,</mo><mi>y</mi></mrow><mrow><mi>T</mi><mo>−</mo><msub><mi>T</mi><mn>0</mn></msub></mrow></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">w_{x,y} = \pi_{\theta}(y)f(r_{x,y}^{T-T_0})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2244em;vertical-align:-0.3831em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.453em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.1389em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3831em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>；期望计算基于来自先前策略 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><msub><mi>θ</mi><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub></msub></mrow><annotation encoding="application/x-tex">\pi_{\theta_{old}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6864em;vertical-align:-0.2559em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:-0.0278em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2559em;"><span></span></span></span></span></span></span></span></span></span> 的样本 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>。函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span></span></span></span> 的选择可对应不同的策略梯度变体：例如，选择截断函数可得到 PPO，选择组标准化奖励可得到 GRPO。该公式即使在存在非可微奖励的情况下，仍能实现稳定且低方差的梯度估计，同时为离散扩散模型的微调提供了灵活的目标函数。在多个离散生成任务上的数值实验表明，SEPO 具有良好的可扩展性和效率，验证了策略梯度强化学习能可靠地应用于离散扩散模型。</p>
<p>d1[42] 为掩码扩散语言模型提出了一个两阶段后训练框架，该框架 <span style="color:lightgreen">将监督微调 (SFT) 与一种新型策略梯度算法 diffu-GRPO 相结合。</span> 由于扩散语言模型缺乏可分解的似然函数，为将 GRPO 适配到扩散语言模型中，d1 提出了 <span style="color:lightgreen">序列对数概率和令牌级对数概率的新型估计方法</span> ：通过简单的平均场分解，将序列对数概率近似为独立令牌级概率的乘积；而令牌级对数概率的计算方式为：在每次策略梯度更新时，以随机掩码的提示 (prompt) 为条件，对完全掩码的补全内容 (completion) 执行单次前向传播。在每次内部梯度更新步骤中，为提示采用不同的随机掩码，这一操作可作为一种正则化手段，提升训练效率与稳定性。d1 的完整流程（先执行SFT，再执行diffu-GRPO）表明，LLaDA 模型在数学推理和规划推理任务上的性能得到了显著提升。</p>
<p>“多模态大扩散语言模型”（MMaDA）[31]是一种统一的<span style="color:cyan">多模态扩散模型</span>，其提出了一个三阶段训练流程。在第一阶段预训练完成后，MMaDA 采用 <span style="color:lightgreen">混合长思维链微调</span> 策略：将不同任务的推理轨迹整理为统一格式，以对齐跨模态的推理过程。这一操作为第三阶段的训练奠定了基础——该阶段引入了 UniGRPO，这是一种专为扩散语言模型设计的策略梯度强化学习算法。</p>
<p>UniGRPO 通过一种结构化加噪策略克服了 d1 等基准方法的局限性：该策略对掩码率 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">p_i \in [0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> 进行均匀采样，而非对所有响应令牌进行掩码。这确保模型能接触到多步扩散去噪过程的各个阶段（从几乎完全掩码到几乎无掩码），与传统扩散训练保持一致，同时提升了模型多步去噪能力的利用率。此外，UniGRPO 通过对掩码令牌取平均，近似计算序列级对数似然。</p>
<p>DiffuCoder[84] 是一款专为代码生成设计并分析的 7B 参数扩散语言模型。该研究提出了一种名为<span style="color:cyan">coupled-GRPO 的强化学习算法</span>，该算法通过利用 DLMs 生成过程的独特属性，实现了“扩散原生”设计。coupled-GRPO 的核心创新在于其用于对数似然估计的耦合采样方案。为获得更稳健、更低方差的估计结果，该算法会为训练批次中的每个补全序列构建成对的互补掩码。对于给定序列，生成的两个掩码需满足：每个令牌位置恰好仅在其中一个掩码中被掩盖。随后，通过对这两次互补前向传播的损失取平均，得到对数概率估计值。这一设计确保训练过程中每个令牌都能在部分掩码场景下被评估，与采用单一随机掩码或全掩码的方法相比，既实现了令牌的全面覆盖，又能获得更稳定的梯度信号。实验表明，coupled-GRPO 显著提升了 DiffuCoder 在代码生成任务上的性能，同时还促使模型形成更偏向并行、更少依赖自回归的生成模式。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510192204035.png" srcset="/img/loading.gif" lazyload alt="Table 2" width="720" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Table 2
    </span>
  </div>
</div>
<h4 id="3-2-3-Adapting-Preference-Optimization-to-DLMs">3.2.3 Adapting Preference Optimization to DLMs</h4>
<blockquote>
<p>偏好优化方法向扩散语言模型的适配</p>
</blockquote>
<p>LLaDA 1.5[85]提出了一种名为 <span style="color:lightgreen">方差缩减偏好优化</span> (Variance Reduced Preference Optimization, VRPO) 的新型框架，旨在将偏好优化方法适配到离散扩散语言模型中。该研究指出，由于用于近似对数似然的证据下界 (Evidence Lower Bound, ELBO) 存在高方差问题，将直接偏好优化 (Direct Preference Optimization, DPO) 应用于离散扩散语言模型面临挑战。</p>
<p>为解决这一问题，VRPO 引入了两种关键的 <span style="color:lightgreen">无偏方差缩减技术：</span></p>
<ol>
<li>蒙特卡洛采样预算的最优分配：通过采样更多扩散时间步（而非为每个时间步采样多个掩码版本）实现，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mi>t</mi></msub><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n_t = n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span> 且 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><msub><mi>y</mi><mi>t</mi></msub></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n_{y_t} = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>；</li>
<li>对偶采样（Antithetic Sampling）：对于相同输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">y_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 或 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">y_l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，当前策略 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 与参考策略 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mrow><mi>r</mi><mi>e</mi><mi>f</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\pi_{ref}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">re</span><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 的ELBO估计共享相同的时间步和掩码数据。</li>
</ol>
<p>将 VRPO 应用于 LLaDA 后，得到的 LLaDA 1.5 模型在数学推理、代码生成及对齐基准测试中均展现出显著且稳定的性能提升。</p>
<h2 id="4-Inference-Strategies">4 Inference Strategies</h2>
<blockquote>
<p>推理策略</p>
</blockquote>
<p>DLMs 的推理策略旨在实现三大核心目标：</p>
<ol>
<li>通过 <span style="color:lightgreen">解掩码</span> (unmasking) 与 <span style="color:lightgreen">重掩码</span> (remasking) 调度等方式提升生成质量；</li>
<li>实现更精细的内容控制；</li>
<li>通过键值 (KV) / 特征缓存、步骤蒸馏等技术提升效率。</li>
</ol>
<p>相关方法的简要概述如图 5 所示。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510192209993.png" srcset="/img/loading.gif" lazyload alt="Fig.5" width="720" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Fig.5: 扩散语言模型的推理技术。此处我们展示了六种不同策略，包括：(a) 并行解码；(b) 去掩码&重掩码；(c) 无分类器引导；(d) 键值缓存；(e) 特征缓存；以及(f) 步数蒸馏。
    </span>
  </div>
</div>
<h3 id="4-1-Parallel-Decoding">4.1 Parallel Decoding</h3>
<blockquote>
<p>并行解码</p>
</blockquote>
<p>并行解码与扩散语言模型的特性天然契合——利用模型固有的掩码预测能力，可同时生成多个令牌（而非按序列生成）。然而，简单粗暴的并行化可能会降低文本连贯性，因此研究人员提出了一系列自适应策略，以平衡效率与生成质量。</p>
<ul>
<li>Fast-dLLM[44] 采用 <span style="color:lightgreen">置信感知解码</span>(confidence-aware decoding) ：仅对预测概率超过阈值的令牌进行解掩码，在不损失生成质量的前提下，实现了最高 27.6 倍的速度提升；</li>
<li>自适应并行解码 (Adaptive Parallel Decoding, APD) [45]：通过咨询轻量级自回归辅助模型，动态调整并行化程度，从而在必要时以吞吐量换取生成保真度；</li>
<li>快慢采样 (Slow-Fast Sampling) [86]：引入两阶段调度——首先通过 “慢阶段” 谨慎定位稳定令牌，再通过 “快阶段” 批量确定令牌，结合缓存技术时可实现最高 34 倍的加速；</li>
<li>SpecDiff[78] 进一步提升吞吐量：将离散扩散模型作为全并行 “草稿生成器”，其输出由大型自回归模型快速验证（必要时进行修正），相比基础自回归生成，速度提升最高达 7.2 倍；</li>
<li>视觉-语言模型 Dimple[30] 采用 <span style="color:lightgreen">置信并行解码</span> (confident parallel decoding) ：动态调整每一步揭示的令牌数量，将生成迭代次数减少 1.5 至 7 倍。</li>
</ul>
<p>总体而言，这些并行解码方法在保留（部分场景下甚至提升）生成质量的同时，大幅缩小了扩散模型与自回归模型之间的推理延迟差距。</p>
<h3 id="4-2-Unmasking-Remasking">4.2 Unmasking / Remasking</h3>
<blockquote>
<p>解掩码/重掩码</p>
</blockquote>
<p>当前主流的开源离散扩散语言模型（如LLaDA[28]、Dream[26]）均采用“掩码预测范式”：在每个扩散步骤中，模型会对高置信度令牌进行解掩码，并对不确定位置重新掩码，通过迭代优化序列。因此，解掩码/重掩码策略的选择（即低置信度采样、随机选择或自适应温度）对生成质量和收敛速度均起决定性作用，使其成为最重要的推理调控手段之一。</p>
<p>早期研究 Masked DLM [63] 确立了两种基础策略：<span style="color:lightgreen">随机重掩码与按置信度排序的重掩码。</span> 研究表明，优先处理低置信度位置无需额外成本即可提升生成质量。基于这一发现，Fast-dLLM[44] 提出 <span style="color:lightgreen">置信感知并行解码</span> ：每一步仅对预测概率超过全局阈值的所有位置进行解掩码，在保持准确率的同时实现了最高 13 倍的速度提升。</p>
<p>近期提出的 ReMDM[46] 则设计了一种原理性的推理时重掩码采样器，该采样器可对已解码的令牌重新掩码以进一步优化；通过调整重掩码预算，ReMDM 实现了计算量与质量之间的平滑权衡，并在固定计算量下缩小了与自回归模型的质量差距。</p>
<p>总体而言，这些自适应解掩码/重掩码策略显著提升了扩散语言模型的效率与生成质量，且能与后文将讨论的缓存、步骤蒸馏等独立加速技术无缝集成。</p>
<h3 id="4-3-Guidance">4.3 Guidance</h3>
<blockquote>
<p>引导技术</p>
</blockquote>
<p><span style="color:cyan">引导技术</span> (Guidance) 是扩散模型中关键的推理手段，其通过将生成轨迹导向期望属性来提升输出质量。在扩散模型中，引导技术泛指所有能修改模型去噪轨迹、使样本满足特定条件（如文本提示、类别标签或风格属性）的方法。</p>
<p>这一理念因 分类器引导”[17] 得以普及：该方法将外部分类器的梯度添加到得分估计中，推动样本向目标类别靠拢。此后不久，“无分类器引导”（classifier-free guidance, CFG）[87] 摒弃了对额外分类器的依赖——模型仅需一次性完成“带条件”与“无条件”两种模式的训练，推理时将两种得分估计结合即可：</p>
<p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable width="100%"><mtr><mtd width="50%"></mtd><mtd><mrow><msub><mi>s</mi><mrow><mi>g</mi><mi>u</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>d</mi></mrow></msub><mo>=</mo><msub><mi>s</mi><mrow><mi>u</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>d</mi></mrow></msub><mo>+</mo><mi>λ</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>d</mi></mrow></msub><mo>−</mo><msub><mi>s</mi><mrow><mi>u</mi><mi>n</mi><mi>c</mi><mi>o</mi><mi>n</mi><mi>d</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mtd><mtd width="50%"></mtd><mtd><mtext>(13)</mtext></mtd></mtr></mtable><annotation encoding="application/x-tex">s_{guided}=s_{uncond}+\lambda(s_{cond}-s_{uncond}) \tag{13}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">gu</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">co</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">λ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">co</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">co</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="tag"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord text"><span class="mord">(</span><span class="mord"><span class="mord">13</span></span><span class="mord">)</span></span></span></span></span></span></p>
<p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span> 为引导系数，用于平衡对条件的保真度与样本多样性。这一简洁公式现已成为多数文本到图像系统（如Stable Diffusion[8]）的核心，并被扩散语言模型采纳以实现“提示控制生成”。</p>
<p>后续研究从多个维度对无分类器引导进行了优化：</p>
<ul>
<li>dropout 增强型无分类器引导 (dropout-augmented CFG)：平滑质量-多样性曲线；</li>
<li>基于粒子的引导 (particle-based guidance)：融合多种条件；</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">p^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>加权法 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">p^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>-weighting)：重新缩放噪声项以稳定高 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span> 值下的采样过程。</li>
</ul>
<p>在文本领域，新型引导方案进一步将控制范围扩展到结构与语义约束：</p>
<ul>
<li>FreeCache[88]：将轻量级自回归验证器与离散扩散语言模型结合，验证器在令牌最终确定前对草稿令牌进行批准（或否决），既保证了文本连贯性，又支持激进的特征缓存；</li>
<li>DINGO[89]：将正则表达式控制构建为基于确定有限自动机（DFA）的动态规划搜索，在不改变模型分布的前提下确保约束满足。</li>
</ul>
<p>在其他离散扩散语言模型中，引导技术还可应用于每个扩散步骤，并可选择性地与掩码/重掩码或缓存结合，在保持效率的同时实现对内容（如主题、情感）的调控。总体而言，引导技术已成为扩散模型推理的核心支柱，为使模型输出与用户意图对齐提供了轻量、可调节的手段。</p>
<h3 id="4-4-Efficient-Inference">4.4 Efficient Inference</h3>
<blockquote>
<p>高效推理</p>
</blockquote>
<p>近期的主流扩散语言模型[26]、[32]、[85]将标准 Transformer 架构[147] 与扩散过程的逐步随机推理流程相结合。因此，提升扩散语言模型推理速度的研究主要集中于两种互补策略：</p>
<ol>
<li>通过键值 (KV) 缓存、特征缓存等技术降低 Transformer 骨干网络的单步计算开销；</li>
<li>通过 <span style="color:lightgreen">步骤蒸馏</span> 等技术减少扩散采样的总步数。</li>
</ol>
<h4 id="4-4-1-Key–Value-Cache">4.4.1 Key–Value Cache</h4>
<blockquote>
<p>键值缓存</p>
</blockquote>
<p>传统键值缓存依赖大型语言模型（LLMs）严格的自回归解码模式，因此不适合扩散语言模型的双向、多步生成范式[48]。但近期研究表明，通过精心设计解码调度，可在扩散模型中充分发挥键值缓存的优势：</p>
<ul>
<li>块级扩散 (Block Diffusion) [76]：提出 <span style="color:lightgreen">块级离散去噪扩散语言模型</span> (BD3-LMs)，该模型以自回归方式对粗粒度块进行文本解码，同时在每个块内部执行扩散过程；块生成完成后，其键值对将被冻结并复用，支持变长生成且实现了可量化的速度提升；</li>
<li>Fast-dLLM[44]：保留块级视角并新增 <span style="color:lightgreen">无训练近似双缓存</span> (DualCache)，该缓存利用前缀与后缀令牌在连续扩散步骤中键值激活值的近似一致性，在 LLaDA 与 Dream 模型上实现了最高 27 倍的端到端吞吐量提升，且准确率损失低于 1%；</li>
<li>延迟键值缓存 (dKV-Cache)[48]：发现令牌表示仅在位置解码后才会稳定，因此设计了“延迟条件缓存”——将键值对延迟一步存储；该设计在上述模型上实现了 2-10 倍的速度提升，且质量损失可忽略不计。</li>
</ul>
<p>这些结果表明，半自回归调度与延迟缓存技术为扩散模型的双向条件建模与原本为自回归设计的 Transformer 优化手段之间搭建了实用桥梁。</p>
<h4 id="4-4-2-Feature-Cache">4.4.2 Feature Cache</h4>
<blockquote>
<p>特征缓存</p>
</blockquote>
<p><span style="color:cyan">特征缓存</span> 由 DeepCache[90]首次提出，其利用连续扩散步骤中 U-Net 中间层激活值的高度相似性避免冗余计算。后续研究（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi></mrow><annotation encoding="application/x-tex">\Delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span></span></span></span>-DiT[91]、学习式缓存 (Learning-to-Cache) [92]、FasterCache[93]）表明，该原理可无缝迁移到基于 Transformer 的扩散模型中，无需重新训练即可实现相当的速度提升。</p>
<p>随着扩散语言模型的兴起，扩散语言模型缓存 (dLLM-Cache)[47] 将特征缓存扩展到文本领域，其核心是区分两种冗余性：</p>
<ul>
<li>提示令牌（prompt tokens）：在整个去噪过程中几乎保持不变；</li>
<li>响应令牌（response tokens）：仅发生稀疏变化。</li>
</ul>
<p>基于此，dLLM-Cache 将“长间隔提示缓存”与“自适应短间隔响应缓存”结合——仅当轻量级值验证（V-verify）检测到显著变化时，才刷新响应缓存。该方法在 LLaDA-8B 与 Dream-7B 模型上实现了最高 9 倍的端到端速度提升。</p>
<p>近期提出的 FreeCache[88]进一步优化：缓存已“纯净”令牌的键值/特征投影，仅刷新动态位置，在保持生成保真度的同时将加速比提升至 34 倍。这些进展表明，特征缓存可使扩散语言模型的推理延迟逼近自回归大型语言模型，且无需牺牲输出质量。</p>
<h4 id="4-4-3-Step-Distillation">4.4.3 Step Distillation</h4>
<blockquote>
<p>步骤蒸馏</p>
</blockquote>
<p><span style="color:cyan">步骤蒸馏</span> 是扩散模型中广泛采用的加速技术，其将典型的千步去噪过程压缩到仅几步（甚至单步），从而大幅缩短推理时间。与前文讨论的无训练方法不同，步骤蒸馏存在离线成本：需先训练一个紧凑的学生网络来模仿教师网络。</p>
<p>早期研究（如渐进式蒸馏 (Progressive Distillation)[95]）及后续的 ADD[148]、LADD[149] 通过逐步减半步数或对齐中间分布来保留生成保真度。Di4C[94] 将该框架扩展到离散扩散领域，通过显式蒸馏令牌间相关性，训练出步数为 4-10 步、质量与教师网络相当的学生网络，同时实现约 2 倍的速度提升。</p>
<p>近期的 DLM-One[43] 采用 <span style="color:lightgreen">带对抗正则化的基于得分的蒸馏</span>，训练出可通过单次前向传播生成完整序列的连续扩散语言模型，实现了最高 500倍的加速，且质量接近教师网络。这些研究共同确立了步骤蒸馏在缩小扩散模型与自回归模型推理延迟差距中的核心地位。</p>
<h2 id="5-Multimodel-and-Unified-Approaches">5 Multimodel and Unified Approaches</h2>
<blockquote>
<p>多模态与统一方法</p>
</blockquote>
<p>本节探讨将 DLMs 扩展至多模态与统一架构的最新进展。与自回归大型语言模型 (AR LLMs) 类似，扩散语言模型可自然适配多模态输入与输出任务。一种直接的方法是通过预训练视觉编码器接收视觉输入：借鉴自回归领域中 LLaVA[150] 的成功经验，LLaDA-V[29]、LaViDa[96]、Dimple[30] 等模型均采用 <span style="color:lightgreen">视觉编码器提取图像特征，并将其投影到与文本令牌相同的嵌入空间中。</span></p>
<p>除简单的视觉理解外，扩散语言模型还为统一多模态生成与理解提供了极具潜力的路径。得益于其共享的去噪扩散框架，扩散语言模型天然支持对不同模态的联合建模：通过向量量化变分自编码器 (VQ-VAE) 对视觉输入进行离散化处理，可在统一的令牌空间中完成对多模态输入输出的训练。MMaDA[31]、Fudoki[97]、Muddit[80]等代表性模型均是这一方向的典型案例。</p>
<h3 id="5-1-LLaDA-and-LLaDA’s-Derivatives">5.1 LLaDA and LLaDA’s Derivatives</h3>
<blockquote>
<p>LLaDA 及其衍生模型</p>
</blockquote>
<p>首先介绍 LLaDA[28] 系列及其衍生模型，这类模型均基于基础 LLaDA 模型的架构与预训练权重构建。其中，LLaDA-V[29] 将视觉编码器与基于多层感知机 (MLP) 的投影器相结合，该投影器可将视觉特征映射到文本令牌的嵌入空间，从而实现有效的视觉指令微调。</p>
<p>借鉴 LLaVA-NeXT[151] 的思路，LLaDA-V 采用三阶段微调策略：</p>
<ol>
<li>第一阶段：仅训练 MLP 投影器，利用 LLaVA 的训练数据将视觉表征与文本嵌入对齐；</li>
<li>第二阶段：基于扩散语言模型的目标函数，使用大规模视觉指令数据[152]对模型进行进一步微调；</li>
<li>第三阶段：通过对含推理链的问答 (QA) 样本进行训练，提升模型的多模态推理能力。</li>
</ol>
<p>尽管在纯文本任务上，LLaDA 的骨干网络性能略逊于 LLaMA3-8B[153]，但在相同训练数据下，LLaDA-V 在各类基准测试中均展现出更优的性能与可扩展性：不仅缩小了与 Qwen2-VL[154] 的性能差距，还优于混合模态与纯扩散语言模型（如[79]、[155]、[156]），充分证明了扩散架构在多模态理解任务中的有效性。</p>
<h3 id="5-2-LaViDa">5.2 LaViDa</h3>
<p>LaViDa[96] 提出了一系列基于 LLaDA 与 Dream-7B[26] 的视觉语言模型 (VLM)。该模型同样采用预训练视觉编码器，并通过两阶段训练策略分别训练投影器与微调模型，在解决多模态扩散语言模型的训练与推理挑战方面做出了显著贡献。</p>
<p>在典型的掩码扩散语言模型中，平均仅约 50% 的令牌会参与损失计算，这不仅降低了训练效率，还可能在 VLM 训练中遗漏关键答案令牌，进而导致梯度失配。为此，LaViDa 提出了 <span style="color:lightgreen">互补掩码策略</span> 以实现高效训练：对每个样本生成两个含“不重叠受损片段”的掩码版本，确保所有令牌最终均能参与训练，从而提升样本利用率与梯度流动效率。</p>
<p>在推理阶段，LaViDa 采用 <span style="color:lightgreen">前缀键值缓存</span> (Prefix KV-Cache) 对视觉令牌与提示令牌的键值对进行缓存，在仅造成微小性能损失的前提下，显著降低了推理延迟，最高加速比达 3.9 倍。此外，该模型还通过 <span style="color:lightgreen">时间步偏移</span> (timestep shifting) 提前对令牌进行解掩码，进一步提升了生成质量。实验结果表明，LaViDa 的性能可与基于自回归 (AR)的视觉语言模型媲美甚至更优，同时还具备显著的推理加速优势。</p>
<h3 id="5-3-MMaDA">5.3 MMaDA</h3>
<p>基于 LLaDA，MMaDA[31] 进一步将架构泛化，以同时支持多模态理解与生成任务。与此前模型不同，MMaDA 无需显式视觉编码器：通过向量量化变分自编码器 (VQ-VAE) 将图像 token 化为离散编码，并利用 <span style="color:lightgreen">模态无关的扩散Transformer</span> 对所有模态进行联合建模。这种设计无需模态专用组件，即可实现文本与图像模态的无缝融合。</p>
<p>此外，MMaDA 还采用了 <span style="color:lightgreen">混合长思维链微调策略</span>，使跨模态的思维链推理格式保持对齐；同时，针对扩散语言模型定制了基于统一策略梯度的强化学习算法 UniGRPO，从而实现跨模态推理。在性能上，MMaDA 不仅在文本推理任务中超越了 LLaMA3 等同等规模模型，在多模态理解任务中超越了 Show-o[155]，甚至在图像生成任务中优于 SDXL[10] 等专业图像生成模型。</p>
<h3 id="5-4-Dimple">5.4 Dimple</h3>
<p>Dimple[30]提出了一种大型多模态扩散语言模型，将视觉编码器与离散扩散语言模型骨干网络相结合。研究人员发现，纯离散扩散训练方法存在显著的不稳定性、性能不佳与严重的长度偏差问题。为克服这些挑战，Dimple提出了一种名为 <span style="color:lightgreen">先自回归后扩散</span> (Autoregressive-then-Diffusion) 的新型两阶段训练范式：</p>
<ol>
<li>第一阶段：对模型进行标准自回归训练，以实现视觉与语言模态的有效对齐；</li>
<li>第二阶段：切换至基于扩散的训练，以恢复模型的并行解码能力。</li>
</ol>
<p>这种混合策略在确保训练稳定高效的同时，使模型性能达到甚至超越了 LLaVA-NEXT 等当代自回归模型。</p>
<p>在推理阶段，Dimple引入了多种技术以提升效率与可控性：</p>
<ul>
<li>置信解码 (Confident Decoding)：基于置信度阈值动态调整每一步生成的令牌数量，减少总生成迭代次数；</li>
<li>预填充技术 (Prefilling)：成功复现了自回归模型中常用的预填充技术，通过缓存提示令牌实现最高 7 倍的加速，且性能损失极小；</li>
<li>结构先验 (Structure Priors)：可对响应格式与长度进行精确、细粒度的控制，而这一特性在自回归模型中难以实现。</li>
</ul>
<h3 id="5-5-D-DiT">5.5 D-DiT</h3>
<p>双扩散 Transformer (Dual Diffusion Transformer, D-DiT)[79] 是一种大规模全端到端统一多模态扩散模型，支持文本到图像 (T2I) 与图像到文本 (I2T) 双向任务。该模型直接解决了此前扩散模型在视觉理解任务中面临的挑战——这类任务此前主要由自回归模型主导。</p>
<p>D-DiT 的架构灵感来源于多模态扩散 Transformer (MM-DiT)，采用双分支 Transformer 分别处理图像与文本令牌，且每一层均通过注意力机制实现模态间的交互。该模型使用冻结的 VAE 处理图像，使用冻结的 T5 编码器处理文本，其核心骨干网络 MM-DiT 则基于预训练 SD3[11] 的权重初始化。</p>
<p>D-DiT 的核心创新在于其 <span style="color:lightgreen">联合训练目标</span>：通过联合优化图像与文本两种模态的损失之和，将图像的连续 latent 空间扩散与文本的离散掩码令牌扩散相结合。与此前需通过自回归组件解码文本 latent 的多模态扩散模型不同，D-DiT 是完全基于扩散的模型，且与其他统一模型相比展现出了极具竞争力的性能。</p>
<h3 id="5-6-UniDisc">5.6 UniDisc</h3>
<p>统一多模态离散扩散模型 (Unified Multimodal Discrete Diffusion, UniDisc)[98] 是一种面向文本与图像联合建模的统一生成模型，其以离散扩散为基础，作为 AR 方法的替代方案。与前文讨论的 D-DiT 不同，UniDisc <span style="color:lightgreen">对文本和图像令牌联合执行完整的掩码扩散过程，并采用全注意力机制</span>，旨在学习将掩码令牌序列从共享词汇表映射回清洁令牌序列。</p>
<p>UniDisc 的训练从零开始，采用统一的离散扩散目标函数：<span style="color:lightgreen">对两种模态的令牌进行随机掩码，并通过重加权交叉熵损失对模型进行监督。</span> 该模型的核心优势在于其在条件生成任务中的卓越性能，这在很大程度上归功于对无分类器引导 (classifier-free guidance) 的有效运用。</p>
<p>UniDisc 最显著的能力之一是支持 <span style="color:lightgreen">零样本方式的图像与文本联合补全</span>——这一特性是此前自回归模型或其他统一生成模型无法实现的。研究人员通过将模型参数规模扩展至 1.4B 进行缩放分析，结果表明：UniDisc 在性能与推理时计算效率两方面均优于自回归模型，同时还具备更强的可控性与可编辑性。不过，在达到相同验证损失的前提下，UniDisc 的训练效率低于性能相当的自回归模型。</p>
<h3 id="5-7-Fudoki">5.7 Fudoki</h3>
<p>Fudoki[97] 是首个完全基于离散流匹配框架构建的通用统一多模态模型，其问世对 AR 模型与 Masked DLMs 的主导地位构成了挑战。与依赖简单掩码加噪过程的模型不同，Fudoki采用了更通用的 <span style="color:lightgreen">度量诱导概率路径</span> 与 <span style="color:lightgreen">动态最优速度</span>，这不仅使加噪过程在语义层面更具意义，还能让模型在迭代优化过程中持续自我修正预测结果。</p>
<p>这种自我修正能力是 Fudoki 与 masked DLMs 的核心区别——在掩码扩散语言模型中，已解掩码的令牌通常是固定的，无法修改。为降低从零训练的高昂成本，Fudoki 从基于自回归的预训练多模态大型语言模型 (MLLM) Janus-1.5B[157] 初始化，随后通过两阶段过程适配到离散流匹配范式。</p>
<p>其架构以 Janus-1.5B 为基础，但采用全注意力掩码以更好地捕捉全局上下文，并移除了时间嵌入层（因模型可从受损输入中隐式推断时间步）。在视觉理解与图像生成任务中，Fudoki 的性能均可与最先进的 AR 模型媲美，同时在推理速度与生成质量之间实现了灵活权衡。此外，当应用测试时推理缩放技术时，该模型性能显著提升，这表明该架构在下一代统一模型研发中具有进一步探索的潜力。</p>
<h3 id="5-8-Muddit模型">5.8 Muddit模型</h3>
<p>Muddit[80]是一种纯统一离散扩散 Transformer，其将强大的文本到图像骨干网络与轻量级文本解码器相结合，在真正统一的架构下实现了灵活且高质量的多模态生成。该模型从Meissonic[145] 的预训练多模态扩散 Transformer (MM-DiT) 初始化，训练过程采用统一的离散扩散目标函数：根据余弦调度对文本和图像令牌进行随机掩码，模型通过重加权交叉熵损失学习预测原始令牌。</p>
<p>借助语义丰富的视觉先验与并行离散扩散的双重优势，Muddit 在生成与理解类基准测试中，性能达到甚至超越了规模显著更大的自回归模型；同时，其相比自回归基准模型实现了数倍加速——这一结果凸显了在初始化得当的情况下，离散扩散方法具备高效性与可扩展性。</p>
<h2 id="6-Performance-Study">6 Performance Study</h2>
<blockquote>
<p>性能研究</p>
</blockquote>
<p>在本节中，我们简要对比了各类 DLMs 与 ARMs 的性能。我们基于多个广泛用于评估扩散语言模型的基准测试展开可视化分析，具体包括：用于通用语言理解的 PIQA（物理常识推理基准）[158] 和 HellaSwag（常识推理基准）[159]、用于代码生成的 HumanEval（代码生成评估基准）[160]，以及用于多模态生成与理解的 GenEval（生成质量评估基准）[161]、MME（多模态理解评估基准）[162]、MMMU（多学科多模态理解推理基准）[163] 和 GQA（视觉问答基准）[164]。此外，我们还纳入了 GSM8K[165]——这是扩散语言模型相关文献中用于评估数学推理能力的常用基准测试。相应的性能可视化结果如图 6 所示。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510192212209.png" srcset="/img/loading.gif" lazyload alt="Fig.6" width="720" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Fig.6: 八个基准测试的性能对比：总体生成评估(Overall-GenEval)、多模态评估(MME)、常识问答(CQA)、Hellaswag、物理推理问答(PIQA)、人类评估(HumanEval)、GSM8K数学题以及多学科多模态理解(MMMU)。每个子图中的横轴代表模型参数量级，纵轴表示对应基准测试下的得分，分数越高反映性能越优。模型类型通过颜色区分：蓝色代表自回归语言模型(AR)，橙色代表扩散语言模型(DLM)。
    </span>
  </div>
</div>
<p>本次研究涉及的扩散语言模型，其参数规模涵盖不足 1B 到 8B。为便于对比，我们还报告了相同参数规模下代表性自回归模型的性能。性能数据主要来源于原始文献；若原始文献中未提供相关结果，则参考后续发表的、包含可比评估结果的研究。</p>
<p>研究结果表明，DLMs 的性能总体上可与相同参数规模的 ARMs 媲美：</p>
<ul>
<li>在 PIQA、HellaSwag 等通用语言理解基准测试中，LLaDA 等扩散语言模型的性能略低于或持平于 LLaMA2[4]、Qwen2.5[166] 等自回归模型；</li>
<li>但在 GSM8K、GPQA[167]、MATH[168] 等数学与科学相关基准测试中，扩散语言模型表现更优，其中 LLaDA、Dream 等模型持续优于相同规模的自回归模型；</li>
<li>在多模态任务中，MMaDA[31]、LLaDA-V[29] 等扩散语言模型往往超越基于自回归架构的多模态模型，凸显了扩散语言模型在统一多模态推理与跨模态推理中的潜力；</li>
<li>在代码生成任务中，扩散语言模型也展现出极具竞争力的能力。值得注意的是，在开源模型中，DiffuCoder[84] 在 HumanEval 基准测试中取得了极具竞争力的性能，这体现了扩散语言模型在结构化、高逻辑性领域的应用潜力；</li>
<li>此外，Gemini Diffusion[33]、Mercury[32] 等闭源扩散语言模型在所有扩散语言模型中取得了最先进的结果，可与 GPT-4o 等顶级自回归模型相媲美。</li>
</ul>
<p>考虑到当前大多数扩散语言模型的训练所使用的训练数据和计算资源相对有限，上述结果表明，在众多实际应用场景中，扩散语言模型具有成为自回归模型可行替代方案的巨大潜力。</p>
<h2 id="7-Application-on-Downstream-Tasks">7 Application on Downstream Tasks</h2>
<blockquote>
<p>下游任务应用</p>
</blockquote>
<h3 id="7-1-Conventional-NLP-Tasks">7.1 Conventional NLP Tasks</h3>
<blockquote>
<p>传统自然语言处理任务</p>
</blockquote>
<p>在用于通用语言生成的大规模扩散语言模型（DLMs）出现之前，扩散语言模型已被应用于各类传统自然语言处理（NLP）任务，例如文本分类[99]、命名实体/场景识别[100]、[101]、情感分析[102]、文档摘要[103]、[104]、风格迁移[110]、[169]、约束生成[111]–[115]以及机器翻译[116]、[170]等。</p>
<p>ROIC-DM[99] 是首个将扩散模型适配于稳健文本分类与推理任务的研究。该模型将扩散过程直接应用于类别标签，并以输入文本为条件约束去噪过程；此外，还可通过将传统语言模型作为“顾问”融入其中，进一步提升模型性能。DiffusionNER[100] 将命名实体识别 (Named Entity Recognition) 构建为边界去噪任务：对实体的起始和终止边界应用扩散过程，通过迭代优化从随机噪声中生成实体跨度。</p>
<p>在场景文本识别任务中，IPAD[101] 提出了一种并行迭代网络，将该任务构建为条件文本生成问题；该网络采用离散扩散与“先易后难”解码方法，有效平衡了识别准确率与推理速度。在基于方面的情感分析 (aspect-based sentiment analysis) 任务中，DiffusionABSA[102]采用扩散模型逐步渐进地提取情感方面。</p>
<p>DiffuSum[103] 为抽取式摘要任务提出了一种新范式：利用扩散模型直接生成目标摘要句的表示，随后通过抽取与这些生成表示最匹配的文档句子，形成最终摘要。针对法律文档摘要任务，TermDiffuSum[104] 提出了一种术语引导的扩散模型——通过多因素融合噪声加权调度，优先处理包含法律术语的句子。</p>
<p>在关键词提取任务中，Diff-KPE[105] 借助变分信息瓶颈 (Variational Information Bottleneck，VIB) 引导文本扩散过程，生成并注入关键词信息，从而增强短语表示。IPED[106] 将关系三元组提取视为一项隐式块扩散任务。EdiText[107] 则提出了一种可控的粗到精文本编辑框架，通过整合基于 SDEdit 的技术与新型自条件机制，实现对文本编辑的精确控制。为生成更具针对性的共情回复，DIFFUSEMP[108] 采用条件扩散模型，该模型由多粒度控制信号（如意图和语义框架）引导，这些信号通过特殊的掩码策略进行整合。DiffuDetox[109] 采用混合扩散方法实现文本净化：将用于降低毒性的条件模型与用于确保输出文本流畅性的无条件模型相结合。</p>
<p>研究表明，经过微调的 DiffuSeq 模型在细粒度文本风格迁移任务上实现了最先进的性能[169]；而 ParaGuide[110] 则提出了一种更灵活的即插即用框架——在推理阶段，通过现成的分类器和风格嵌入器，对“以释义为条件”的扩散模型进行引导。为生成流畅多样且无重复的段落，PLANNER[111] 将“潜在扩散规划模块”与“自回归解码模块”相结合：前者用于生成段落语义嵌入，后者用于生成最终文本。</p>
<p>针对约束极强的诗歌生成任务，PoetryDiffusion[115] 采用独特的任务拆分方式：由扩散模型负责生成语义内容，同时由一个独立训练的新型韵律控制器 (metrical controller) 来强制执行格式、押韵等结构规则。在机器翻译领域，XDLM[116] 开创性地为扩散模型设计了跨语言预训练目标，使模型能在预训练阶段有效学习语言间的映射关系。</p>
<p>DiffusionRet[117] 提出了一种两阶段生成式检索方法：首先利用扩散模型从查询语句生成伪文档，随后将该伪文档作为输入，通过基于 n 元语法 (n-gram) 的模型检索最终文档。DIFND[118] 采用扩散模型生成辟谣证据，并结合多智能体多模态大型语言模型 (MLLM) 系统执行“链式辟谣推理”，以提升多模态假新闻检测的准确率与可解释性。</p>
<h3 id="7-2-Code-Generation">7.2 Code Generation</h3>
<blockquote>
<p>代码生成</p>
</blockquote>
<p>尽管 DLMs 很少被明确设计用于代码生成，但其具备的全局规划与迭代优化能力，却特别适合代码生成的“非序列性”特点。目前已出现专为该领域设计的基础模型，例如 7B 参数的开源模型 DiffuCoder[84]。</p>
<p>对 DiffuCoder 的分析揭示了其独特的解码行为，例如在较高温度 (temperature) 设置下，生成顺序会变得更灵活。该模型还提出了一种名为 coupled-GRPO 的新型采样方案：为训练中使用的补全序列构建互补掩码噪声，这一设计显著提升了模型在代码生成任务上的性能。</p>
<p>在推理能力层面，DCoLT[82] 将整个逆向扩散过程视为一种非线性“横向”思维：通过基于结果的 RL 与解掩码策略模块，该模型在复杂代码生成任务上取得了优异成绩。扩张解掩码调度器(Dilated Unmasking Scheduler, DUS)[119]则提出了一种 <span style="color:lightgreen">仅推理、无规划器</span> 的方法：通过非相邻模式对令牌进行解掩码，以最小化每个去噪步骤中联合熵增益的上界，在改善速度-质量权衡关系的同时，在代码生成任务上取得了良好效果。</p>
<p>为体现扩散语言模型在速度上的实际应用潜力，Mercury Coder[32]作为一款商用级扩散模型，实现了最先进的吞吐量——在主流代码基准测试中，其性能与速度优化后的 ARMs 相当，但速度却比后者快至多 10 倍。</p>
<h3 id="7-3-Biological-and-Scientific-Applications">7.3 Biological and Scientific Applications</h3>
<blockquote>
<p>生物与科学应用</p>
</blockquote>
<p>TransDLM[120] 以目标属性的文本描述为引导进行分子优化，从而避免误差传播。另一类以文本为引导的方法 TGM-DLM[121] 则聚焦于分子生成：通过对 SMILES 字符串（简化分子输入线输入系统，用于表示分子结构的文本格式）的令牌嵌入进行集体迭代更新来生成分子。在不依赖额外数据资源的情况下，TGM-DLM 的生成性能超过了 MolT5-Base模型。</p>
<p>DRAKES[125] 为离散扩散模型提出了一种基于 RL 的微调方法：利用 Gumbel-Softmax 技巧实现奖励反向传播，适用于 DNA 与蛋白质设计任务。在蛋白质建模领域，ForceGen[126] 借助蛋白质语言扩散模型生成符合复杂非线性力学属性设计目标的序列，从而实现从头蛋白质设计。</p>
<p>MeMDLM[122] 通过微调 ESM-2 蛋白质语言模型，提出了一种用于从头膜蛋白设计的掩码扩散语言模型，可生成新颖且真实的跨膜序列。受 LLaDA 启发，DSM[127] 提出了一种兼具高质量表征学习与高效蛋白质生成设计能力的模型。</p>
<p>DPLM[123] 是一款多功能蛋白质语言模型，对蛋白质序列具有强大的生成与预测能力，且在表征学习方面表现优异。DPLM2[128] 进一步将该模型扩展为多模态蛋白质基础模型，可同时处理蛋白质序列与结构：通过将 3D 结构坐标转换为离散令牌，DPLM2 学习这两种模态的联合分布，不仅能同时协同生成匹配的蛋白质序列及其 3D 结构，还支持蛋白质折叠、逆折叠等条件任务。</p>
<p>CFP-GEN[124]是一款专为 <span style="color:lightgreen">组合功能蛋白质生成</span> 设计的新型扩散语言模型。该模型通过整合功能、序列、结构等多模态约束，助力从头蛋白质设计；不仅支持高通量生成功能与天然蛋白质相当的新型蛋白质，还在多功能蛋白质设计中实现了较高的成功率。</p>
<h2 id="8-Challenges-and-Future-Directions">8 Challenges and Future Directions</h2>
<p>尽管 DLMs 已在各类任务中展现出巨大潜力，但仍存在若干关键挑战，限制了其实际部署与更广泛应用。本节将概述并探讨需进一步研究与创新的关键领域。</p>
<h3 id="8-1-Major-Challenges">8.1 Major Challenges</h3>
<h4 id="8-1-1-Parallelism–Performance-Trade-off">8.1.1 Parallelism–Performance Trade-off</h4>
<blockquote>
<p>并行性-性能权衡</p>
</blockquote>
<p>扩散语言模型的设计目标是并行生成多个 tokens。然而，这种并行性往往需要以生成质量和连贯性为代价。在离散扩散语言模型中，单一步骤同时对多个令牌进行解掩码会增加去噪负担，可能导致误差累积。<span style="color:red">核心问题</span> 在于 <span style="color:lightgreen">tokens 间的相互依赖关系</span>，这一问题被称为 <strong>Parallel Decoding Curse</strong> [44]。当模型同时预测多个令牌时，会为每个位置生成概率分布并独立采样，无法考虑位置间的依赖关系。</p>
<p>举一个简单例子：若训练数据仅包含两个序列（“ABABAB” 和 “BABABA”），从统计上看，“A” 和 “B” 在训练数据的每个位置出现频率相同，这会导致扩散语言模型在预测时为两者分配相近的概率。而在 ARMs 中，一旦生成第一个 “A”，模型接下来大概率会预测 “B”，从而保持序列连贯性；相比之下，并行生成 tokens 的 DLMs 可能会为第一个和第二个位置独立采样 “A”，生成 “AAABBA” 这类偏离有效训练模式的序列。实证研究表明，该问题对扩散语言模型的性能影响显著，尤其是在减少去噪步数时[27]。图 7 对这一现象进行了可视化展示。未来研究可聚焦于缓解这一权衡关系，潜在方向包括引入结构化约束、更明确地建模令牌间依赖关系，或优化采样策略以提升并行生成时的序列连贯性。</p>
<div style="display: flex; flex-wrap: wrap; justify-content: center;">
  <div style="margin: 10px;text-align: center;">
    <img src="https://raw.githubusercontent.com/DBQDSS/Blog_img/study/202510192227219.png" srcset="/img/loading.gif" lazyload alt="Fig.7" width="720" height="">
    <br>
    <span style="color: lightpink; font-size: 16px;">
    Fig.7: LLaDA[28] 与 MMaDA[31] 在不同去噪步数设置下的生成结果。需注意 LLaDA 和 MMaDA 的生成长度分别设置为 128 tokens 和 256 tokens。两种模型仅当每步解掩1 or 2 tokens 时才能生成正确连贯的响应。随着步数减少和并行度增加，生成的响应要么错误，要么缺乏流畅性与一致性。这揭示了深度学习模型在并行度与输出质量之间存在的权衡关系。为简化展示，我们省略了 MMaDA 在 256 steps 时的部分思维过程。
    </span>
  </div>
</div>
<h4 id="8-1-2-Infrastructure">8.1.2 Infrastructure</h4>
<blockquote>
<p>基础设施</p>
</blockquote>
<p>ARMs 的训练、微调和推理已通过开源、高度优化的库与框架（如 Hugging Face Transformers[171]）得到显著简化与加速，但 DLMs 在这方面仍相对滞后。目前，主流机器学习生态系统对 DLMs 的原生支持极少甚至完全没有，给研究人员与开发者带来了实际挑战。此外，在推理阶段，DLMs 缺乏类似 vLLM[172] 的成熟开源部署基础设施，难以实现高效服务。</p>
<h4 id="8-1-3-Long-Sequence-and-Dynamic-Length-Generation">8.1.3 Long Sequence and Dynamic-Length Generation</h4>
<blockquote>
<p>长序列与动态长度生成</p>
</blockquote>
<p>DLMs 通常在基于扩散的目标函数下，针对固定长度序列进行去噪训练，这使其在推理阶段难以泛化到更长或动态长度的序列。目前大多数 DLMs 的最大上下文长度限制在 4096 tokens，且 ARMs 中广泛用于长序列的外推技术，在 DLMs 领域仍未得到充分探索。这一限制阻碍了 DLMs 在需要长上下文理解或复杂推理的任务中的应用。此外，DLMs 通常需要在推理阶段预先确定生成长度，使其难以适配动态长度生成场景。尽管 DLMs 可预测 [EOS] token（句末令牌）并省略后续生成的 tokens，但无论生成在逻辑上是否已结束，整个序列仍会在整个去噪过程中被完整更新，这导致了不必要的计算开销。</p>
<p>与此同时，Masked DLMs 在每个去噪步骤中均采用 <span style="color:lightgreen">全双向注意力机制</span>，每一步的计算复杂度为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>（其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 为序列长度）。假设每一步解掩码的 token 数量固定，去噪步骤的总次数会随 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 线性增长，最终导致整体推理复杂度达到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>3</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。若缺乏键值缓存 (KV-Cache) 等架构层面的优化，这种三次方时间复杂度会严重限制 DLMs 在实际应用中处理长序列生成任务的可扩展性。</p>
<h4 id="8-1-4-Scalability">8.1.4 Scalability</h4>
<blockquote>
<p>可扩展性</p>
</blockquote>
<p>对于 DLMs 而言，可扩展性仍是一个未被充分探索的挑战，尤其与 ARMs 相比差距明显。尽管 DLMs 在部分指标与基准测试中展现出良好潜力，但其规模尚未达到 ARMs 的水平：目前公开可用的最大 DLMs 仅含约 8B 参数，远小于已突破千亿甚至万亿参数规模的主流自回归模型（如Llama-3.1-405B[153]、DeepSeek-V3-671B-A37B MoE[16]、Qwen3-235B-A22B MoE[173]、Kimi K2-1T-A32B MoE[174]等）。即便在闭源领域，Mercury、Gemini Diffusion等 DLMs 在各类基准测试中，性能也仍落后于最先进的自回归模型。</p>
<p>此外，现有许多 DLMs 要么基于预训练自回归模型初始化，要么以基础 DLMs （如LLaDA）为基础、使用有限数据集训练，这进一步限制了其可扩展性与性能上限。因此， DLMs 的进一步规模化能力仍需验证与探索。</p>
<h3 id="8-2-未来方向">8.2 未来方向</h3>
<p>尽管面临上述挑战，DLMs 仍为未来研究提供了诸多极具潜力的方向。下文将简要概述几个尚未充分探索、且可能显著推动该领域发展的方向与机遇：</p>
<ul>
<li>训练效率 (Training Efficiency)：当前 DLMs 的训练效率普遍低于自回归模型，原因包括损失计算过程中令牌利用率有限等。未来研究可探索混合 DLMs 架构或改进训练方案，以实现与自回归模型相当甚至更优的训练效率。</li>
<li>量化与二值化 (Quantization and Binaryization)（Low-bit DLMs ）：低比特量化与二值化技术在自回归模型中已得到广泛研究，但在 DLMs 中仍未充分探索。将这些技术适配到扩散范式中，有望提升推理速度并降低内存消耗，为 DLMs 在实际系统中的部署提供便利。</li>
<li>剪枝与蒸馏 (Pruning and Distillation)：剪枝、知识蒸馏等模型压缩技术已成功应用于自回归模型，有效减小了模型规模并降低了推理成本。将这些技术应用于 DLMs ，可提升其部署能力，尤其适用于资源受限或延迟敏感的场景。</li>
<li>多模态统一推理 (Multimodal Unified Reasoning)：尽管近期多模态 DLMs 在跨模态理解与生成任务中展现出令人印象深刻的能力，但多数模型仍局限于单次单模态推理。未来可聚焦于构建“真正一体化”的统一 DLMs ，使其能够跨多个模态执行复杂推理。</li>
<li>基于 DLMs 的智能体 (DLM-based Agents)：DLMs 在驱动智能体方面的潜力仍未被充分挖掘。借助其双向上下文建模、并行解码与迭代优化能力，基于 DLMs 的智能体有望在动态环境中具备更高的灵活性与适应性，成为传统基于自回归模型的智能体方案的有力替代选择。</li>
</ul>
<h2 id="9-Conclusion">9 Conclusion</h2>
<p>本综述对 DLMs 的整体领域进行了深入概述。我们阐述了 DLMs 的基本原理、分类体系与建模范式，并将其与主流自回归模型进行对比，突出了 DLMs 的独特特征与优势。我们进一步探索了训练与推理的设计空间，涵盖了旨在提升生成质量与效率的各类训练策略及推理技术。此外，我们还重点介绍了多模态 DLMs 的最新进展，展示了其处理多样化数据模态的能力。最后，我们讨论了该领域存在的局限性与挑战，并概述了具有前景的未来研究方向。</p>
<p>我们希望本综述能为对基于扩散的语言建模感兴趣的研究人员提供全面参考，为其了解该领域的当前状况与未来前景提供有价值的见解。我们也鼓励研究人员在这一充满活力的研究领域中进一步探索与创新，推动 DLMs 不断发展，突破语言理解与生成的边界。</p>
<h2 id="参考文献">参考文献</h2>
<ul class="references-list" style="list-style-type: none; padding-left: 0;">
  <li id="ref1" style="margin-bottom: 10px;">
    <strong>[1] Language models are few-shot learners</strong> 
    <br>
    <span style="margin-left: 1.5em;">T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.</span>
  </li>
  <li id="ref2" style="margin-bottom: 10px;">
    <strong>[2] Gpt-4 technical report</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al. arXiv preprint, <a href="https://arxiv.org/abs/2303.08774" target="_blank">arXiv:2303.08774</a>, 2023.</span>
  </li>
  <li id="ref3" style="margin-bottom: 10px;">
    <strong>[3] Palm: Scaling language modeling with pathways</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al. Journal of Machine Learning Research, vol. 24, no. 240, pp. 1–113, 2023.</span>
  </li>
  <li id="ref4" style="margin-bottom: 10px;">
    <strong>[4] Llama: Open and efficient foundation language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al. arXiv preprint, <a href="https://arxiv.org/abs/2302.13971" target="_blank">arXiv:2302.13971</a>, 2023.</span>
  </li>
  <li id="ref5" style="margin-bottom: 10px;">
    <strong>[5] Qwen technical report</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al. arXiv preprint, <a href="https://arxiv.org/abs/2309.16609" target="_blank">arXiv:2309.16609</a>, 2023.</span>
  </li>
  <li id="ref6" style="margin-bottom: 10px;">
    <strong>[6] A survey of large language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al. arXiv preprint, <a href="https://arxiv.org/abs/2303.18223" target="_blank">arXiv:2303.18223</a>, vol. 1, no. 2, 2023.</span>
  </li>
  <li id="ref7" style="margin-bottom: 10px;">
    <strong>[7] Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al. arXiv preprint, <a href="https://arxiv.org/abs/2501.12948" target="_blank">arXiv:2501.12948</a>, 2025.</span>
  </li>
  <li id="ref8" style="margin-bottom: 10px;">
    <strong>[8] High-resolution image synthesis with latent diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10684–10695.</span>
  </li>
  <li id="ref9" style="margin-bottom: 10px;">
    <strong>[9] Photorealistic text-to-image diffusion models with deep language understanding</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans et al. Advances in neural information processing systems, vol. 35, pp. 36479–36494, 2022.</span>
  </li>
  <li id="ref10" style="margin-bottom: 10px;">
    <strong>[10] Sdxl: Improving latent diffusion models for high-resolution image synthesis</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. Podell, J. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, Z. Penna, and R. Rombach. in The Twelfth International Conference on Learning Representations.</span>
  </li>
  <li id="ref11" style="margin-bottom: 10px;">
    <strong>[11] Scaling rectified flow transformers for high-resolution image synthesis</strong> 
    <br>
    <span style="margin-left: 1.5em;">P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel et al. in Forty-first international conference on machine learning, 2024.</span>
  </li>
  <li id="ref12" style="margin-bottom: 10px;">
    <strong>[12] Video generation models as world simulators</strong> 
    <br>
    <span style="margin-left: 1.5em;">T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman et al. OpenAI Blog, vol. 1, p. 8, 2024.</span>
  </li>
  <li id="ref13" style="margin-bottom: 10px;">
    <strong>[13] Improving language understanding by generative pre-training</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. 2018.</span>
  </li>
  <li id="ref14" style="margin-bottom: 10px;">
    <strong>[14] Language models are unsupervised multitask learners</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. OpenAI blog, vol. 1, no. 8, p. 9, 2019.</span>
  </li>
  <li id="ref15" style="margin-bottom: 10px;">
    <strong>[15] Gemini: a family of highly capable multimodal models</strong> 
    <br>
    <span style="margin-left: 1.5em;">G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican et al. arXiv preprint, <a href="https://arxiv.org/abs/2312.11805" target="_blank">arXiv:2312.11805</a>, 2023.</span>
  </li>
  <li id="ref16" style="margin-bottom: 10px;">
    <strong>[16] Deepseek-v3 technical report</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al. arXiv preprint, <a href="https://arxiv.org/abs/2412.19437" target="_blank">arXiv:2412.19437</a>, 2024.</span>
  </li>
  <li id="ref17" style="margin-bottom: 10px;">
    <strong>[17] Diffusion models beat gans on image synthesis</strong> 
    <br>
    <span style="margin-left: 1.5em;">P. Dhariwal and A. Nichol. Advances in neural information processing systems, vol. 34, pp. 8780–8794, 2021.</span>
  </li>
  <li id="ref18" style="margin-bottom: 10px;">
    <strong>[18] Denoising diffusion probabilistic models</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Ho, A. Jain, and P. Abbeel. Advances in neural information processing systems, vol. 33, pp. 6840–6851, 2020.</span>
  </li>
  <li id="ref19" style="margin-bottom: 10px;">
    <strong>[19] Denoising diffusion implicit models</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Song, C. Meng, and S. Ermon. arXiv preprint, <a href="https://arxiv.org/abs/2010.02502" target="_blank">arXiv:2010.02502</a>, 2020.</span>
  </li>
  <li id="ref20" style="margin-bottom: 10px;">
    <strong>[20] Score-based generative modeling through stochastic differential equations</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. in International Conference on Learning Representations.</span>
  </li>
  <li id="ref21" style="margin-bottom: 10px;">
    <strong>[21] Flow straight and fast: Learning to generate and transfer data with rectified flow</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Liu, C. Gong et al. in The Eleventh International Conference on Learning Representations.</span>
  </li>
  <li id="ref22" style="margin-bottom: 10px;">
    <strong>[22] Diffusion-lm improves controllable text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Li, J. Thickstun, I. Gulrajani, P. S. Liang, and T. B. Hashimoto. Advances in neural information processing systems, vol. 35, pp. 4328–4343, 2022.</span>
  </li>
  <li id="ref23" style="margin-bottom: 10px;">
    <strong>[23] Self-conditioned embedding diffusion for text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">R. Strudel, C. Tallec, F. Altché, Y. Du, Y. Ganin, A. Mensch, W. Grathwohl, N. Savinov, S. Dieleman, L. Sifre et al. arXiv preprint, <a href="https://arxiv.org/abs/2211.04236" target="_blank">arXiv:2211.04236</a>, 2022.</span>
  </li>
  <li id="ref24" style="margin-bottom: 10px;">
    <strong>[24] Structured denoising diffusion models in discrete state-spaces</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg. Advances in neural information processing systems, vol. 34, pp. 17981–17993, 2021.</span>
  </li>
  <li id="ref25" style="margin-bottom: 10px;">
    <strong>[25] Diffusionbert: Improving generative masked language models with diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. He, T. Sun, Q. Tang, K. Wang, X. Huang, and X. Qiu. in The 61st Annual Meeting Of The Association For Computational Linguistics, 2023.</span>
  </li>
  <li id="ref26" style="margin-bottom: 10px;">
    <strong>[26] Dream 7b</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Ye, L. Xie, L. Zheng, J. Gao, Z. Wu, X. Jiang, Z. Li, and Z. Kong. 2025. [Online]. Available: <a href="https://hkunlp.github.io/blog/2025/dream" target="_blank">https://hkunlp.github.io/blog/2025/dream</a></span>
  </li>
  <li id="ref27" style="margin-bottom: 10px;">
    <strong>[27] Scaling diffusion language models via adaptation from autoregressive models</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Gong, S. Agarwal, Y. Zhang, J. Ye, L. Zheng, M. Li, C. An, P. Zhao, W. Bi, J. Han et al. in The Thirteenth International Conference on Learning Representations.</span>
  </li>
  <li id="ref28" style="margin-bottom: 10px;">
    <strong>[28] Large language diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Nie, F. Zhu, C. Li, X. Zhang, J. Ou, J. Hu, J. Zhou, Y. Lin, J.-R. Wen, and Z. You. arXiv preprint, <a href="https://arxiv.org/abs/2502.09992" target="_blank">arXiv:2502.09992</a>, 2025.</span>
  </li>
  <li id="ref29" style="margin-bottom: 10px;">
    <strong>[29] Llada-v: Large language diffusion models with visual instruction tuning</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. You, S. Nie, X. Zhang, J. Hu, J. Zhou, Z. Lu, J.-R. Wen, and C. Li. arXiv preprint, <a href="https://arxiv.org/abs/2505.16933" target="_blank">arXiv:2505.16933</a>, 2025.</span>
  </li>
  <li id="ref30" style="margin-bottom: 10px;">
    <strong>[30] Dimple: Discrete diffusion multimodal large language model with parallel decoding</strong> 
    <br>
    <span style="margin-left: 1.5em;">R. Yu, X. Ma, and X. Wang. arXiv preprint, <a href="https://arxiv.org/abs/2505.16990" target="_blank">arXiv:2505.16990</a>, 2025.</span>
  </li>
  <li id="ref31" style="margin-bottom: 10px;">
    <strong>[31] Mmada: Multimodal large diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">L. Yang, Y. Tian, B. Li, X. Zhang, K. Shen, Y. Tong, and M. Wang. arXiv preprint, <a href="https://arxiv.org/abs/2505.15809" target="_blank">arXiv:2505.15809</a>, 2025.</span>
  </li>
  <li id="ref32" style="margin-bottom: 10px;">
    <strong>[32] Mercury: Ultra-fast language models based on diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">I. Labs, S. Khanna, S. Kharbanda, S. Li, H. Varma, E. Wang, S. Birnbaum, Z. Luo, Y. Miraoui, A. Palrecha et al. arXiv preprint, <a href="https://arxiv.org/abs/2506.17298" target="_blank">arXiv:2506.17298</a>, 2025.</span>
  </li>
  <li id="ref33" style="margin-bottom: 10px;">
    <strong>[33] Gemini diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">DeepMind. 2024, accessed: 2025-07-09. <a href="https://deepmind.google.com/technologies/gemini" target="_blank">https://deepmind.google.com/technologies/gemini</a></span>
  </li>
  <li id="ref34" style="margin-bottom: 10px;">
    <strong>[34] Energy-based diffusion language models for text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">M. Xu, T. Gehner, K. Kreis, W. Nie, Y. Xu, J. Leskovec, S. Ermon, and A. Vahdat. arXiv preprint, <a href="https://arxiv.org/abs/2410.21357" target="_blank">arXiv:2410.21357</a>, 2024.</span>
  </li>
  <li id="ref35" style="margin-bottom: 10px;">
    <strong>[35] Beyond autoregression: Fast llms via self-distillation through time</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Deschenaux and C. Gulcehre. in The Thirteenth International Conference on Learning Representations.</span>
  </li>
  <li id="ref36" style="margin-bottom: 10px;">
    <strong>[36] Transfer learning for text diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">K. Han, K. Kenealy, A. Barua, N. Fiedel, and N. Constant. arXiv preprint, <a href="https://arxiv.org/abs/2401.17181" target="_blank">arXiv:2401.17181</a>, 2024.</span>
  </li>
  <li id="ref37" style="margin-bottom: 10px;">
    <strong>[37] The diffusion duality</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. S. Sahoo, J. Deschenaux, A. Gokaslan, G. Wang, J. Chiu, and V. Kuleshov. arXiv preprint, <a href="https://arxiv.org/abs/2506.10892" target="_blank">arXiv:2506.10892</a>, 2025.</span>
  </li>
  <li id="ref38" style="margin-bottom: 10px;">
    <strong>[38] Non-markovian discrete diffusion with causal language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Zhang, S. He, D. Levine, L. Zhao, D. Zhang, S. A. Rizvi, E. Zappala, R. Ying, and D. van Dijk. arXiv preprint, <a href="https://arxiv.org/abs/2502.09767" target="_blank">arXiv:2502.09767</a>, 2025.</span>
  </li>
  <li id="ref39" style="margin-bottom: 10px;">
    <strong>[39] Inference-time scaling of diffusion language models with particle gibbs sampling</strong> 
    <br>
    <span style="margin-left: 1.5em;">M. Dang, J. Han, M. Xu, K. Xu, A. Srivastava, and S. Ermon. arXiv preprint, <a href="https://arxiv.org/abs/2507.08390" target="_blank">arXiv:2507.08390</a>, 2025.</span>
  </li>
  <li id="ref40" style="margin-bottom: 10px;">
    <strong>[40] Anchored diffusion language model</strong> 
    <br>
    <span style="margin-left: 1.5em;">L. Rout, C. Caramanis, and S. Shakkottai. arXiv preprint, <a href="https://arxiv.org/abs/2505.18456" target="_blank">arXiv:2505.18456</a>, 2025.</span>
  </li>
  <li id="ref41" style="margin-bottom: 10px;">
    <strong>[41] Deepseekmath: Pushing the limits of mathematical reasoning in open language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al. arXiv preprint, <a href="https://arxiv.org/abs/2402.03300" target="_blank">arXiv:2402.03300</a>, 2024.</span>
  </li>
  <li id="ref42" style="margin-bottom: 10px;">
    <strong>[42] dl: Scaling reasoning in diffusion large language models via reinforcement learning</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Zhao, D. Gupta, Q. Zheng, and A. Grover. arXiv preprint, <a href="https://arxiv.org/abs/2504.12216" target="_blank">arXiv:2504.12216</a>, 2025.</span>
  </li>
  <li id="ref43" style="margin-bottom: 10px;">
    <strong>[43] Dlm-one: Diffusion language models for zero-step sequence generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">T. Chen, S. Zhang, and M. Zhou. arXiv e-prints, pp. arXiv-2506, 2025.</span>
  </li>
  <li id="ref44" style="margin-bottom: 10px;">
    <strong>[44] Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. Wu, H. Zhang, S. Xue, Z. Liu, S. Diao, L. Zhu, P. Luo, S. Han, and E. Xie. arXiv preprint, <a href="https://arxiv.org/abs/2505.22618" target="_blank">arXiv:2505.22618</a>, 2025.</span>
  </li>
  <li id="ref45" style="margin-bottom: 10px;">
    <strong>[45] Accelerating diffusion llms via adaptive parallel decoding</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. Israel, G. V. d. Broeck, and A. Grover. arXiv preprint, <a href="https://arxiv.org/abs/2506.00413" target="_blank">arXiv:2506.00413</a>, 2025.</span>
  </li>
  <li id="ref46" style="margin-bottom: 10px;">
    <strong>[46] Remasking discrete diffusion models with inference-time scaling</strong> 
    <br>
    <span style="margin-left: 1.5em;">G. Wang, Y. Schiff, S. S. Sahoo, and V. Kuleshov. ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy.</span>
  </li>
  <li id="ref47" style="margin-bottom: 10px;">
    <strong>[47] dllm-cache: Accelerating diffusion large language models with adaptive caching</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Liu, Y. Yang, Y. Zhang, J. Chen, C. Zou, Q. Wei, S. Wang, and L. Zhang. arXiv preprint, <a href="https://arxiv.org/abs/2506.06295" target="_blank">arXiv:2506.06295</a>, 2025.</span>
  </li>
  <li id="ref48" style="margin-bottom: 10px;">
    <strong>[48] dkv-cache: The cache for diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Ma, R. Yu, G. Fang, and X. Wang. arXiv preprint, <a href="https://arxiv.org/abs/2505.15781" target="_blank">arXiv:2505.15781</a>, 2025.</span>
  </li>
  <li id="ref49" style="margin-bottom: 10px;">
    <strong>[49] Composable text controls in latent space with odes</strong> 
    <br>
    <span style="margin-left: 1.5em;">G. Liu, Z. Feng, Y. Gao, Z. Yang, X. Liang, J. Bao, X. He, S. Cui, Z. Li, and Z. Hu. arXiv preprint, <a href="https://arxiv.org/abs/2208.00638" target="_blank">arXiv:2208.00638</a>, 2022.</span>
  </li>
  <li id="ref50" style="margin-bottom: 10px;">
    <strong>[50] Diffuseq: Sequence to sequence text generation with diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong. The Eleventh International Conference on Learning Representations.</span>
  </li>
  <li id="ref51" style="margin-bottom: 10px;">
    <strong>[51] Continuous diffusion for categorical data</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Dieleman, L. Sartran, A. Roshannai, N. Savinov, Y. Ganin, P. H. Richemond, A. Doucet, R. Strudel, C. Dyer, C. Durkan et al. arXiv preprint, <a href="https://arxiv.org/abs/2211.15089" target="_blank">arXiv:2211.15089</a>, 2022.</span>
  </li>
  <li id="ref52" style="margin-bottom: 10px;">
    <strong>[52] Empowering diffusion models on the embedding space for text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Gao, J. Guo, X. Tan, Y. Zhu, F. Zhang, J. Bian, and L. Xu. arXiv preprint, <a href="https://arxiv.org/abs/2212.09412" target="_blank">arXiv:2212.09412</a>, 2022.</span>
  </li>
  <li id="ref53" style="margin-bottom: 10px;">
    <strong>[53] Latent diffusion for language generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Lovelace, V. Kishore, C. Wan, E. Shekhtman, and K. Q. Weinberger. Advances in Neural Information Processing Systems, vol. 36, pp. 56998–57025, 2023.</span>
  </li>
  <li id="ref54" style="margin-bottom: 10px;">
    <strong>[54] Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Lin, Y. Gong, Y. Shen, T. Wu, Z. Fan, C. Lin, N. Duan, and W. Chen. International Conference on Machine Learning. PMLR, 2023, pp. 21051–21064.</span>
  </li>
  <li id="ref55" style="margin-bottom: 10px;">
    <strong>[55] Infodiffusion: Information entropy aware diffusion process for non-autoregressive text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">R. Wang, W. Li, and P. Li. Findings of the Association for Computational Linguistics: EMNLP 2023, 2023, pp. 13757–13770.</span>
  </li>
  <li id="ref56" style="margin-bottom: 10px;">
    <strong>[56] Unified generation, reconstruction, and representation: Generalized diffusion with adaptive latent encoding-decoding</strong> 
    <br>
    <span style="margin-left: 1.5em;">G. Liu, Y. Wang, Z. Feng, Q. Wu, L. Tang, Y. Gao, Z. Li, S. Cui, J. Mcauley, Z. Yang et al. International Conference on Machine Learning. PMLR, 2024, pp. 31964–31993.</span>
  </li>
  <li id="ref57" style="margin-bottom: 10px;">
    <strong>[57] Smoothie: Smoothing diffusion on token embeddings for text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Shabalin, V. Meshchaninov, and D. Vetrov. arXiv preprint, <a href="https://arxiv.org/abs/2505.18853" target="_blank">arXiv:2505.18853</a>, 2025.</span>
  </li>
  <li id="ref58" style="margin-bottom: 10px;">
    <strong>[58] Tess: Text-to-text self-conditioned simplex diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">R. K. Mahabadi, H. Ivison, J. Tae, J. Henderson, I. Beltagy, M. E. Peters, and A. Cohan. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 2347–2361.</span>
  </li>
  <li id="ref59" style="margin-bottom: 10px;">
    <strong>[59] Tess 2: A large-scale generalist diffusion language model</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Tae, H. Ivison, S. Kumar, and A. Cohan. arXiv preprint, <a href="https://arxiv.org/abs/2502.13917" target="_blank">arXiv:2502.13917</a>, 2025.</span>
  </li>
  <li id="ref60" style="margin-bottom: 10px;">
    <strong>[60] Latent diffusion energy-based model for interpretable text modelling</strong> 
    <br>
    <span style="margin-left: 1.5em;">P. Yu, S. Xie, X. Ma, B. Jia, B. Pang, R. Gao, Y. Zhu, S.-C. Zhu, and Y. N. Wu. International Conference on Machine Learning. PMLR, 2022, pp. 25702–25720.</span>
  </li>
  <li id="ref61" style="margin-bottom: 10px;">
    <strong>[61] A reparameterized discrete diffusion model for text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">L. Zheng, J. Yuan, L. Yu, and L. Kong. First Conference on Language Modeling.</span>
  </li>
  <li id="ref62" style="margin-bottom: 10px;">
    <strong>[62] Simplified and generalized masked diffusion for discrete data</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Shi, K. Han, Z. Wang, A. Doucet, and M. Titsias. Advances in neural information processing systems, vol. 37, pp. 103131–103167, 2024.</span>
  </li>
  <li id="ref63" style="margin-bottom: 10px;">
    <strong>[63] Simple and effective masked diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Sahoo, M. Arriola, Y. Schiff, A. Gokaslan, E. Marroquin, J. Chiu, A. Rush, and V. Kuleshov. Advances in Neural Information Processing Systems, vol. 37, pp. 130136–130184, 2024.</span>
  </li>
  <li id="ref64" style="margin-bottom: 10px;">
    <strong>[64] Diffusion language models can perform many tasks with scaling and instruction-finetuning</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Ye, Z. Zheng, Y. Bao, L. Qian, and Q. Gu. arXiv preprint, <a href="https://arxiv.org/abs/2308.12219" target="_blank">arXiv:2308.12219</a>, 2023.</span>
  </li>
  <li id="ref65" style="margin-bottom: 10px;">
    <strong>[65] Diffusion-nat: Self-prompting discrete diffusion for non-autoregressive text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">K. Zhou, Y. Li, W. X. Zhao, and J.-R. Wen. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 1438–1451.</span>
  </li>
  <li id="ref66" style="margin-bottom: 10px;">
    <strong>[66] Likelihood-based diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">I. Gulrajani and T. B. Hashimoto. Advances in Neural Information Processing Systems, vol. 36, pp. 16693–16715, 2023.</span>
  </li>
  <li id="ref67" style="margin-bottom: 10px;">
    <strong>[67] Discrete diffusion modeling by estimating the ratios of the data distribution</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Lou, C. Meng, and S. Ermon. International Conference on Machine Learning. PMLR, 2024, pp. 32819–32848.</span>
  </li>
  <li id="ref68" style="margin-bottom: 10px;">
    <strong>[68] Your absorbing discrete diffusion secretly models the conditional distributions of clean data</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Ou, S. Nie, K. Xue, F. Zhu, J. Sun, Z. Li, and C. Li. The Thirteenth International Conference on Learning Representations, 2024.</span>
  </li>
  <li id="ref69" style="margin-bottom: 10px;">
    <strong>[69] Discrete flow matching</strong> 
    <br>
    <span style="margin-left: 1.5em;">I. Gat, T. Remez, N. Shaul, F. Kreuk, R. T. Chen, G. Synnaeve, Y. Adi, and Y. Lipman. Advances in Neural Information Processing Systems, vol. 37, pp. 133345–133385, 2024.</span>
  </li>
  <li id="ref70" style="margin-bottom: 10px;">
    <strong>[70] Think while you generate: Discrete diffusion with planned denoising</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Liu, J. Nam, A. Campbell, H. Stark, Y. Xu, T. Jaakkola, and R. Gomez-Bombarelli. The Thirteenth International Conference on Learning Representations, 2024.</span>
  </li>
  <li id="ref71" style="margin-bottom: 10px;">
    <strong>[71] Beyond autoregression: Discrete diffusion for complex reasoning and planning</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Ye, J. Gao, S. Gong, L. Zheng, X. Jiang, Z. Li, and L. Kong. arXiv preprint, <a href="https://arxiv.org/abs/2410.14157" target="_blank">arXiv:2410.14157</a>, 2024.</span>
  </li>
  <li id="ref72" style="margin-bottom: 10px;">
    <strong>[72] Generalized interpolating discrete diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. von Rütte, J. Fluri, Y. Ding, A. Orvieto, B. Schölkopf, and T. Hofmann. Forty-second International Conference on Machine Learning, 2025.</span>
  </li>
  <li id="ref73" style="margin-bottom: 10px;">
    <strong>[73] Longllada: Unlocking long context capabilities in diffusion llms</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Liu, Z. Liu, Z. Huang, Q. Guo, Z. He, and X. Qiu. arXiv preprint, <a href="https://arxiv.org/abs/2506.14429" target="_blank">arXiv:2506.14429</a>, 2025.</span>
  </li>
  <li id="ref74" style="margin-bottom: 10px;">
    <strong>[74] Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Han, S. Kumar, and Y. Tsvetkov. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 11575–11596.</span>
  </li>
  <li id="ref75" style="margin-bottom: 10px;">
    <strong>[75] Ar-diffusion: Auto-regressive diffusion model for text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">T. Wu, Z. Fan, X. Liu, H.-T. Zheng, Y. Gong, J. Jiao, J. Li, J. Guo, N. Duan, W. Chen et al. Advances in Neural Information Processing Systems, vol. 36, pp. 39957–39974, 2023.</span>
  </li>
  <li id="ref76" style="margin-bottom: 10px;">
    <strong>[76] Block diffusion: Interpolating between autoregressive and diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">M. Arriola, A. Gokaslan, J. T. Chiu, Z. Yang, Z. Qi, J. Han, S. S. Sahoo, and V. Kuleshov. The Thirteenth International Conference on Learning Representations.</span>
  </li>
  <li id="ref77" style="margin-bottom: 10px;">
    <strong>[77] Ctrldiff: Boosting large diffusion language models with dynamic block prediction and controllable generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. Huang and H. Tang. arXiv preprint, <a href="https://arxiv.org/abs/2505.14455" target="_blank">arXiv:2505.14455</a>, 2025.</span>
  </li>
  <li id="ref78" style="margin-bottom: 10px;">
    <strong>[78] Speculative diffusion decoding: Accelerating language generation through diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. K. Christopher, B. R. Bartoldson, T. Ben-Nun, M. Cardei, B. Kailkhura, and F. Fioretto. Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2025, pp. 12042–12059.</span>
  </li>
  <li id="ref79" style="margin-bottom: 10px;">
    <strong>[79] Dual diffusion for unified image generation and understanding</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Li, H. Li, Y. Shi, A. B. Farimani, Y. Kluger, L. Yang, and P. Wang. Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 2779–2790.</span>
  </li>
  <li id="ref80" style="margin-bottom: 10px;">
    <strong>[80] Muddit: Liberating generation beyond text-to-image with a unified discrete diffusion model</strong> 
    <br>
    <span style="margin-left: 1.5em;">Q. Shi, J. Bai, Z. Zhao, W. Chai, K. Yu, J. Wu, S. Song, Y. Tong, X. Li, X. Li et al. arXiv preprint, <a href="https://arxiv.org/abs/2505.23606" target="_blank">arXiv:2505.23606</a>, 2025.</span>
  </li>
  <li id="ref81" style="margin-bottom: 10px;">
    <strong>[81] Diffusion of thought: Chain-of-thought reasoning in diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Ye, S. Gong, L. Chen, L. Zheng, J. Gao, H. Shi, C. Wu, X. Jiang, Z. Li, W. Bi et al. Advances in Neural Information Processing Systems, vol. 37, pp. 105345–105374, 2024.</span>
  </li>
  <li id="ref82" style="margin-bottom: 10px;">
    <strong>[82] Reinforcing the diffusion chain of lateral thought with diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Huang, Z. Chen, Z. Wang, T. Li, and G.-J. Qi. arXiv preprint, <a href="https://arxiv.org/abs/2505.10446" target="_blank">arXiv:2505.10446</a>, 2025.</span>
  </li>
  <li id="ref83" style="margin-bottom: 10px;">
    <strong>[83] Fine-tuning discrete diffusion models with policy gradient methods</strong> 
    <br>
    <span style="margin-left: 1.5em;">O. Zekri and N. Boullé. arXiv preprint, <a href="https://arxiv.org/abs/2502.01384" target="_blank">arXiv:2502.01384</a>, 2025.</span>
  </li>
  <li id="ref84" style="margin-bottom: 10px;">
    <strong>[84] Diffucoder: Understanding and improving masked diffusion models for code generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Gong, R. Zhang, H. Zheng, J. Gu, N. Jaitly, L. Kong, and Y. Zhang. arXiv preprint, <a href="https://arxiv.org/abs/2506.20639" target="_blank">arXiv:2506.20639</a>, 2025.</span>
  </li>
  <li id="ref85" style="margin-bottom: 10px;">
    <strong>[85] Llada 1.5: Variance-reduced preference optimization for large language diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">F. Zhu, R. Wang, S. Nie, X. Zhang, C. Wu, J. Hu, J. Zhou, J. Chen, Y. Lin, J.-R. Wen et al. arXiv preprint, <a href="https://arxiv.org/abs/2505.19223" target="_blank">arXiv:2505.19223</a>, 2025.</span>
  </li>
  <li id="ref86" style="margin-bottom: 10px;">
    <strong>[86] Accelerating diffusion large language models with slowfast: The three golden principles</strong> 
    <br>
    <span style="margin-left: 1.5em;">Q. Wei, Y. Zhang, Z. Liu, D. Liu, and L. Zhang. arXiv preprint, <a href="https://arxiv.org/abs/2506.10848" target="_blank">arXiv:2506.10848</a>, 2025.</span>
  </li>
  <li id="ref87" style="margin-bottom: 10px;">
    <strong>[87] Adaptive classifier-free guidance via dynamic low-confidence masking</strong> 
    <br>
    <span style="margin-left: 1.5em;">P. Li, S. Yan, J. Tsai, R. Zhang, R. An, Z. Guo, and X. Gao. arXiv preprint, <a href="https://arxiv.org/abs/2505.20199" target="_blank">arXiv:2505.20199</a>, 2025.</span>
  </li>
  <li id="ref88" style="margin-bottom: 10px;">
    <strong>[88] Accelerating diffusion language model inference via efficient kv caching and guided diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Hu, J. Meng, Y. Akhauri, M. S. Abdelfattah, J.-s. Seo, Z. Zhang, and U. Gupta. arXiv preprint, <a href="https://arxiv.org/abs/2505.21467" target="_blank">arXiv:2505.21467</a>, 2025.</span>
  </li>
  <li id="ref89" style="margin-bottom: 10px;">
    <strong>[89] Dingo: Constrained inference for diffusion llms</strong> 
    <br>
    <span style="margin-left: 1.5em;">T. Suresh, D. Banerjee, S. Ugare, S. Misailovic, and G. Singh. ICML 2025 Workshop on Reliable and Responsible Foundation Models.</span>
  </li>
  <li id="ref90" style="margin-bottom: 10px;">
    <strong>[90] Deepcache: Accelerating diffusion models for free</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Ma, G. Fang, and X. Wang. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 15762–15772.</span>
  </li>
  <li id="ref91" style="margin-bottom: 10px;">
    <strong>[91] Δ-dit: A training-free acceleration method tailored for diffusion transformers</strong> 
    <br>
    <span style="margin-left: 1.5em;">P. Chen, M. Shen, P. Ye, J. Cao, C. Tu, C.-S. Bouganis, Y. Zhao, and T. Chen. arXiv preprint, <a href="https://arxiv.org/abs/2406.01125" target="_blank">arXiv:2406.01125</a>, 2024.</span>
  </li>
  <li id="ref92" style="margin-bottom: 10px;">
    <strong>[92] Learning-to-cache: Accelerating diffusion transformer via layer caching</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Ma, G. Fang, M. Bi Mi, and X. Wang. Advances in Neural Information Processing Systems, vol. 37, pp. 133282–133304, 2024.</span>
  </li>
  <li id="ref93" style="margin-bottom: 10px;">
    <strong>[93] Fastercache: Training-free video diffusion model acceleration with high quality</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Lv, C. Si, J. Song, Z. Yang, Y. Qiao, Z. Liu, and K.-Y. K. Wong. The Thirteenth International Conference on Learning Representations.</span>
  </li>
  <li id="ref94" style="margin-bottom: 10px;">
    <strong>[94] Distillation of discrete diffusion through dimensional correlations</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Hayakawa, Y. Takida, M. Imaizumi, H. Wakaki, and Y. Mitsufuji. Forty-second International Conference on Machine Learning, 2025.</span>
  </li>
  <li id="ref95" style="margin-bottom: 10px;">
    <strong>[95] Progressive distillation for fast sampling of diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">T. Salimans and J. Ho. International Conference on Learning Representations.</span>
  </li>
  <li id="ref96" style="margin-bottom: 10px;">
    <strong>[96] Lavida: A large diffusion language model for multimodal understanding</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Li, K. Kallidromitis, H. Bansal, A. Gokul, Y. Kato, K. Kozuka, J. Kuen, Z. Lin, K.-W. Chang, and A. Grover. arXiv preprint, <a href="https://arxiv.org/abs/2505.16839" target="_blank">arXiv:2505.16839</a>, 2025.</span>
  </li>
  <li id="ref97" style="margin-bottom: 10px;">
    <strong>[97] Fudoki: Discrete flow-based unified understanding and generation via kinetic-optimal velocities</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Wang, Y. Lai, A. Li, S. Zhang, J. Sun, N. Kang, C. Wu, Z. Li, and P. Luo. arXiv preprint, <a href="https://arxiv.org/abs/2505.20147" target="_blank">arXiv:2505.20147</a>, 2025.</span>
  </li>
  <li id="ref98" style="margin-bottom: 10px;">
    <strong>[98] Unified multimodal discrete diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Swerdlow, M. Prabhudesai, S. Gandhi, D. Pathak, and K. Fragkiadaki. arXiv preprint, <a href="https://arxiv.org/abs/2503.20853" target="_blank">arXiv:2503.20853</a>, 2025.</span>
  </li>
  <li id="ref99" style="margin-bottom: 10px;">
    <strong>[99] Roic-dm: Robust text inference and classification via diffusion model</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Yuan, W. Yuan, H. Yin, and T. He. arXiv preprint, <a href="https://arxiv.org/abs/2401.03514" target="_blank">arXiv:2401.03514</a>, 2024.</span>
  </li>
  <li id="ref100" style="margin-bottom: 10px;">
    <strong>[100] Diffusionner: Boundary diffusion for named entity recognition</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 3875–3890.</span>
  </li>
  <li id="ref101" style="margin-bottom: 10px;">
    <strong>[101] Ipad: Iterative, parallel, and diffusion-based network for scene text recognition</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Yang, Z. Qiao, and Y. Zhou. International Journal of Computer Vision, pp. 1–21, 2025.</span>
  </li>
  <li id="ref102" style="margin-bottom: 10px;">
    <strong>[102] Let’s rectify step by step: Improving aspect-based sentiment analysis with diffusion models</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Liu, J. Zhou, Q. Zhu, Q. Chen, Q. Bai, J. Xiao, and L. He. Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 10324–10335.</span>
  </li>
  <li id="ref103" style="margin-bottom: 10px;">
    <strong>[103] Diffusum: Generation enhanced extractive summarization with diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">H. Zhang, X. Liu, and J. Zhang. Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 13089–13100.</span>
  </li>
  <li id="ref104" style="margin-bottom: 10px;">
    <strong>[104] Termdiffusum: a term-guided diffusion model for extractive summarization of legal documents</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Dong, W. Li, Y. Le, Z. Jiang, J. Zhong, and Z. Wang. Proceedings of the 31st international conference on computational linguistics, 2025, pp. 3222–3235.</span>
  </li>
  <li id="ref105" style="margin-bottom: 10px;">
    <strong>[105] Enhancing phrase representation by information bottleneck guided text diffusion process for keyphrase extraction</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Luo, Q. Zhou, and F. Zhou. Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 6036–6047.</span>
  </li>
  <li id="ref106" style="margin-bottom: 10px;">
    <strong>[106] Ibed: An implicit perspective for relational triple extraction based on diffusion model</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Zhao, C. Xu, and B. Jiang. Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2024, pp. 2080–2092.</span>
  </li>
  <li id="ref107" style="margin-bottom: 10px;">
    <strong>[107] Editext: Controllable coarse-to-fine text editing with diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. H. Lee, H. Kim, J. Yeom, and S. Yoon. arXiv preprint, <a href="https://arxiv.org/abs/2502.19765" target="_blank">arXiv:2502.19765</a>, 2025.</span>
  </li>
  <li id="ref108" style="margin-bottom: 10px;">
    <strong>[108] Difusemp: A diffusion model-based framework with multi-grained control for empathetic response generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">G. Bi, L. Shen, Y. Cao, M. Chen, Y. Xie, Z. Lin, and X. He. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 2812–2831.</span>
  </li>
  <li id="ref109" style="margin-bottom: 10px;">
    <strong>[109] Diffudetox: A mixed diffusion model for text detoxification</strong> 
    <br>
    <span style="margin-left: 1.5em;">G. Floto, M. M. A. Pour, P. Farinneya, Z. Tang, A. Pesaranghader, M. Bharadwaj, and S. Sanner. Findings of the Association for Computational Linguistics: ACL 2023, 2023, pp. 7566–7574.</span>
  </li>
  <li id="ref110" style="margin-bottom: 10px;">
    <strong>[110] Paraguide: Guided diffusion paraphrasers for plug-and-play textual style transfer</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Horvitz, A. Patel, C. Callison-Burch, Z. Yu, and K. McKeown. Proceedings of the AAAI conference on artificial intelligence, vol. 38, no. 16, 2024, pp. 18216–18224.</span>
  </li>
  <li id="ref111" style="margin-bottom: 10px;">
    <strong>[111] Planner: Generating diversified paragraph via latent language diffusion model</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Zhang, J. Gu, Z. Wu, S. Zhai, J. Susskind, and N. Jaitly. Advances in Neural Information Processing Systems, vol. 36, pp. 80178–80190, 2023.</span>
  </li>
  <li id="ref112" style="margin-bottom: 10px;">
    <strong>[112] Diffucom: A novel diffusion model for comment generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Liu, P. Cheng, J. Dai, and J. Liu. Knowledge-Based Systems, vol. 281, p. 111069, 2023.</span>
  </li>
  <li id="ref113" style="margin-bottom: 10px;">
    <strong>[113] Diffusiondialog: A diffusion model for diverse dialog generation with latent space</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Xiang, Z. Liu, H. Liu, Y. Bai, J. Cheng, and W. Chen. Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 4912–4921.</span>
  </li>
  <li id="ref114" style="margin-bottom: 10px;">
    <strong>[114] Improved paraphrase generation via controllable latent diffusion</strong> 
    <br>
    <span style="margin-left: 1.5em;">W. Zou, Z. Zhuang, X. Geng, S. Huang, J. Liu, and J. Chen. arXiv preprint, <a href="https://arxiv.org/abs/2404.08938" target="_blank">arXiv:2404.08938</a>, 2024.</span>
  </li>
  <li id="ref115" style="margin-bottom: 10px;">
    <strong>[115] Poetrydiffusion: Towards joint semantic and metrical manipulation in poetry generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Hu, C. Liu, Y. Feng, A. T. Luu, and B. Hooi. Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, 2024, pp. 18279–18288.</span>
  </li>
  <li id="ref116" style="margin-bottom: 10px;">
    <strong>[116] Xdlm: Cross-lingual diffusion language model for machine translation</strong> 
    <br>
    <span style="margin-left: 1.5em;">L. Chen, A. Feng, B. Yang, and Z. Li. arXiv preprint, <a href="https://arxiv.org/abs/2307.13560" target="_blank">arXiv:2307.13560</a>, 2023.</span>
  </li>
  <li id="ref117" style="margin-bottom: 10px;">
    <strong>[117] Diffusionret: Diffusion-enhanced generative retriever using constrained decoding</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Qiao, X. Liu, and S.-H. Na. Findings of the Association for Computational Linguistics: EMNLP 2023, 2023, pp. 9515–9529.</span>
  </li>
  <li id="ref118" style="margin-bottom: 10px;">
    <strong>[118] Debunk and infer: Multimodal fake news detection via diffusion-generated evidence and llm reasoning</strong> 
    <br>
    <span style="margin-left: 1.5em;">K. Yan, M. Liu, Y. Liu, R. Fu, Z. Wen, J. Tao, and X. Liu. arXiv preprint, <a href="https://arxiv.org/abs/2506.21557" target="_blank">arXiv:2506.21557</a>, 2025.</span>
  </li>
  <li id="ref119" style="margin-bottom: 10px;">
    <strong>[119] Plan for speed–dilated scheduling for masked diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">O. Luxembourg, H. Permuter, and E. Nachmani. arXiv preprint, <a href="https://arxiv.org/abs/2506.19037" target="_blank">arXiv:2506.19037</a>, 2025.</span>
  </li>
  <li id="ref120" style="margin-bottom: 10px;">
    <strong>[120] Text-guided multi-property molecular optimization with a diffusion language model</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Xiong, K. Li, J. Chen, H. Zhang, D. Lin, Y. Che, and W. Hu. arXiv preprint, <a href="https://arxiv.org/abs/2410.13597" target="_blank">arXiv:2410.13597</a>, 2024.</span>
  </li>
  <li id="ref121" style="margin-bottom: 10px;">
    <strong>[121] Text-guided molecule generation with diffusion language model</strong> 
    <br>
    <span style="margin-left: 1.5em;">H. Gong, Q. Liu, S. Wu, and L. Wang. Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 1, 2024, pp. 109–117.</span>
  </li>
  <li id="ref122" style="margin-bottom: 10px;">
    <strong>[122] Memdlm: De novo membrane protein design with masked discrete diffusion protein language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Goel, V. Thoutam, E. M. Marroquin, A. Gokaslan, A. Firouzbakht, S. Vincoff, V. Kuleshov, H. T. Kratochvil, and P. Chatterjee. NeurIPS 2024 Workshop on AI for New Drug Modalities.</span>
  </li>
  <li id="ref123" style="margin-bottom: 10px;">
    <strong>[123] Diffusion language models are versatile protein learners</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Wang, Z. Zheng, D. Xue, S. Huang, Q. Gu et al. Forty-first International Conference on Machine Learning.</span>
  </li>
  <li id="ref124" style="margin-bottom: 10px;">
    <strong>[124] Cfp-gen: Combinatorial functional protein generation via diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Yin, C. Zha, W. He, C. Xu, and X. Gao. Forty-second International Conference on Machine Learning.</span>
  </li>
  <li id="ref125" style="margin-bottom: 10px;">
    <strong>[125] Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. Wang, M. Uehara, Y. He, A. Wang, A. Lal, T. Jaakkola, S. Levine, A. Regev, T. Biancalani et al. The Thirteenth International Conference on Learning Representations.</span>
  </li>
  <li id="ref126" style="margin-bottom: 10px;">
    <strong>[126] Forcegen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a language diffusion model</strong> 
    <br>
    <span style="margin-left: 1.5em;">B. Ni, D. L. Kaplan, and M. J. Buehler. Science Advances, vol. 10, no. 6, p. eadl4000, 2024.</span>
  </li>
  <li id="ref127" style="margin-bottom: 10px;">
    <strong>[127] Diffusion sequence models for enhanced protein representation and generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">L. Hallee, N. Rafailidis, D. B. Bichara, and J. P. Gleghorn. arXiv preprint, <a href="https://arxiv.org/abs/2506.08293" target="_blank">arXiv:2506.08293</a>, 2025.</span>
  </li>
  <li id="ref128" style="margin-bottom: 10px;">
    <strong>[128] Dplm-2: A multimodal diffusion protein language model</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Wang, Z. Zheng, F. Ye, D. Xue, S. Huang, and Q. Gu. arXiv preprint, <a href="https://arxiv.org/abs/2410.13782" target="_blank">arXiv:2410.13782</a>, 2024.</span>
  </li>
  <li id="ref129" style="margin-bottom: 10px;">
    <strong>[129] Deep unsupervised learning using nonequilibrium thermodynamics</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. International conference on machine learning. pmlr, 2015, pp. 2256–2265.</span>
  </li>
  <li id="ref130" style="margin-bottom: 10px;">
    <strong>[130] Bert: Pre-training of deep bidirectional transformers for language understanding</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), 2019, pp. 4171–4186.</span>
  </li>
  <li id="ref131" style="margin-bottom: 10px;">
    <strong>[131] Roberta: A robustly optimized bert pretraining approach</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. arXiv preprint, <a href="https://arxiv.org/abs/1907.11692" target="_blank">arXiv:1907.11692</a>, 2019.</span>
  </li>
  <li id="ref132" style="margin-bottom: 10px;">
    <strong>[132] Albert: A lite bert for self-supervised learning of language representations</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. International Conference on Learning Representations.</span>
  </li>
  <li id="ref133" style="margin-bottom: 10px;">
    <strong>[133] Deberta: Decoding-enhanced bert with disentangled attention</strong> 
    <br>
    <span style="margin-left: 1.5em;">P. He, X. Liu, J. Gao, and W. Chen. International Conference on Learning Representations.</span>
  </li>
  <li id="ref134" style="margin-bottom: 10px;">
    <strong>[134] Transformer-xl: Attentive language models beyond a fixed-length context</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. arXiv preprint, <a href="https://arxiv.org/abs/1901.02860" target="_blank">arXiv:1901.02860</a>, 2019.</span>
  </li>
  <li id="ref135" style="margin-bottom: 10px;">
    <strong>[135] Opt: Open pre-trained transformer language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al. arXiv preprint, <a href="https://arxiv.org/abs/2205.01068" target="_blank">arXiv:2205.01068</a>, 2022.</span>
  </li>
  <li id="ref136" style="margin-bottom: 10px;">
    <strong>[136] Better & faster large language models via multi-token prediction</strong> 
    <br>
    <span style="margin-left: 1.5em;">F. Gloeckle, B. Y. Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve. Forty-first International Conference on Machine Learning.</span>
  </li>
  <li id="ref137" style="margin-bottom: 10px;">
    <strong>[137] Sequence to sequence learning with neural networks</strong> 
    <br>
    <span style="margin-left: 1.5em;">I. Sutskever, O. Vinyals, and Q. V. Le. Advances in neural information processing systems, vol. 27, 2014.</span>
  </li>
  <li id="ref138" style="margin-bottom: 10px;">
    <strong>[138] Exploring the limits of transfer learning with a unified text-to-text transformer</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Journal of machine learning research, vol. 21, no. 140, pp. 1–67, 2020.</span>
  </li>
  <li id="ref139" style="margin-bottom: 10px;">
    <strong>[139] Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</strong> 
    <br>
    <span style="margin-left: 1.5em;">M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 7871–7880.</span>
  </li>
  <li id="ref140" style="margin-bottom: 10px;">
    <strong>[140] Seqdiffuseq: Text diffusion with encoder-decoder transformers</strong> 
    <br>
    <span style="margin-left: 1.5em;">H. Yuan, Z. Yuan, C. Tan, F. Huang, and S. Huang. arXiv preprint, <a href="https://arxiv.org/abs/2212.10325" target="_blank">arXiv:2212.10325</a>, 2022.</span>
  </li>
  <li id="ref141" style="margin-bottom: 10px;">
    <strong>[141] Xlnet: Generalized autoregressive pretraining for language understanding</strong> 
    <br>
    <span style="margin-left: 1.5em;">Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Advances in neural information processing systems, vol. 32, 2019.</span>
  </li>
  <li id="ref142" style="margin-bottom: 10px;">
    <strong>[142] Qwen2.5: A party of foundation models</strong> 
    <br>
    <span style="margin-left: 1.5em;">Q. Team. September 2024. [Online]. Available: <a href="https://qwenlm.github.io/blog/qwen2.5/" target="_blank">https://qwenlm.github.io/blog/qwen2.5/</a></span>
  </li>
  <li id="ref143" style="margin-bottom: 10px;">
    <strong>[143] Segment-level diffusion: A framework for controllable long-form generation with diffusion language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Zhu, G. Karadzhov, C. Whitehouse, and A. Vlachos. arXiv preprint, <a href="https://arxiv.org/abs/2412.11333" target="_blank">arXiv:2412.11333</a>, 2024.</span>
  </li>
  <li id="ref144" style="margin-bottom: 10px;">
    <strong>[144] Latent diffusion for document generation with sequential decoding</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Zihuiwen, Y. Elle Michelle, and B. Phil. NeurIPS 2023 Workshop on Diffusion Models, 2023. [Online]. Available: <a href="https://neurips.cc/virtual/2023/74876" target="_blank">https://neurips.cc/virtual/2023/74876</a></span>
  </li>
  <li id="ref145" style="margin-bottom: 10px;">
    <strong>[145] Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Bai, T. Ye, W. Chow, E. Song, Q.-G. Chen, X. Li, Z. Dong, L. Zhu, and S. Yan. The Thirteenth International Conference on Learning Representations, 2024.</span>
  </li>
  <li id="ref146" style="margin-bottom: 10px;">
    <strong>[146] Addressing the training-inference discrepancy in discrete diffusion for text generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">M. Asada and M. Miwa. Proceedings of the 31st International Conference on Computational Linguistics, 2025, pp. 7156–7164.</span>
  </li>
  <li id="ref147" style="margin-bottom: 10px;">
    <strong>[147] Attention is all you need</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Advances in neural information processing systems, vol. 30, 2017.</span>
  </li>
  <li id="ref148" style="margin-bottom: 10px;">
    <strong>[148] Adversarial diffusion distillation</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Sauer, D. Lorenz, A. Blattmann, and R. Rombach. European Conference on Computer Vision. Springer, 2024, pp. 87–103.</span>
  </li>
  <li id="ref149" style="margin-bottom: 10px;">
    <strong>[149] Fast high-resolution image synthesis with latent adversarial diffusion distillation</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Sauer, F. Boesel, T. Dockhorn, A. Blattmann, P. Esser, and R. Rombach. SIGGRAPH Asia 2024 Conference Papers, 2024, pp. 1–11.</span>
  </li>
  <li id="ref150" style="margin-bottom: 10px;">
    <strong>[150] Visual instruction tuning</strong> 
    <br>
    <span style="margin-left: 1.5em;">H. Liu, C. Li, Q. Wu, and Y. J. Lee. Advances in neural information processing systems, vol. 36, pp. 34892–34916, 2023.</span>
  </li>
  <li id="ref151" style="margin-bottom: 10px;">
    <strong>[151] Llava-interleave: Tackling multi-image, video, and 3d in large multimodal models</strong> 
    <br>
    <span style="margin-left: 1.5em;">F. Li, R. Zhang, H. Zhang, Y. Zhang, B. Li, W. Li, Z. Ma, and C. Li. The Thirteenth International Conference on Learning Representations.</span>
  </li>
  <li id="ref152" style="margin-bottom: 10px;">
    <strong>[152] Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Guo, T. Zheng, Y. Bai, B. Li, Y. Wang, K. Zhu, Y. Li, G. Neubig, W. Chen, and X. Yue. arXiv preprint, <a href="https://arxiv.org/abs/2410.05237" target="_blank">arXiv:2410.05237</a>, 2024.</span>
  </li>
  <li id="ref153" style="margin-bottom: 10px;">
    <strong>[153] The llama 3 herd of models</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan et al. arXiv preprint, <a href="https://arxiv.org/abs/2407.21783" target="_blank">arXiv:2407.21783</a>, 2024.</span>
  </li>
  <li id="ref154" style="margin-bottom: 10px;">
    <strong>[154] Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution</strong> 
    <br>
    <span style="margin-left: 1.5em;">P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al. arXiv preprint, <a href="https://arxiv.org/abs/2409.12191" target="_blank">arXiv:2409.12191</a>, 2024.</span>
  </li>
  <li id="ref155" style="margin-bottom: 10px;">
    <strong>[155] Show-o: One single transformer to unify multimodal understanding and generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z. Shou. The Thirteenth International Conference on Learning Representations.</span>
  </li>
  <li id="ref156" style="margin-bottom: 10px;">
    <strong>[156] Orthus: Autoregressive interleaved image-text generation with modality-specific heads</strong> 
    <br>
    <span style="margin-left: 1.5em;">S. Kou, J. Jin, Z. Liu, C. Liu, Y. Ma, J. Jia, Q. Chen, P. Jiang, and Z. Deng. arXiv preprint, <a href="https://arxiv.org/abs/2412.00127" target="_blank">arXiv:2412.00127</a>, 2024.</span>
  </li>
  <li id="ref157" style="margin-bottom: 10px;">
    <strong>[157] Janus: Decoupling visual encoding for unified multimodal understanding and generation</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan et al. Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 12966–12977.</span>
  </li>
  <li id="ref158" style="margin-bottom: 10px;">
    <strong>[158] Piqua: Reasoning about physical commonsense in natural language</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Bisk, R. Zellers, J. Gao, Y. Choi et al. Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 7432–7439.</span>
  </li>
  <li id="ref159" style="margin-bottom: 10px;">
    <strong>[159] Hel-laswag: Can a machine really finish your sentence?</strong> 
    <br>
    <span style="margin-left: 1.5em;">R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 4791–4800.</span>
  </li>
  <li id="ref160" style="margin-bottom: 10px;">
    <strong>[160] Evaluating large language models trained on code</strong> 
    <br>
    <span style="margin-left: 1.5em;">M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al. arXiv preprint, <a href="https://arxiv.org/abs/2107.03374" target="_blank">arXiv:2107.03374</a>, 2021.</span>
  </li>
  <li id="ref161" style="margin-bottom: 10px;">
    <strong>[161] Geneval: An object-focused framework for evaluating text-to-image alignment</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. Ghosh, H. Hajishirzi, and L. Schmidt. Advances in Neural Information Processing Systems, vol. 36, pp. 52132–52152, 2023.</span>
  </li>
  <li id="ref162" style="margin-bottom: 10px;">
    <strong>[162] Mme: A comprehensive evaluation benchmark for multimodal large language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun et al. arXiv preprint, <a href="https://arxiv.org/abs/2306.13394" target="_blank">arXiv:2306.13394</a>, 2023.</span>
  </li>
  <li id="ref163" style="margin-bottom: 10px;">
    <strong>[163] Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi</strong> 
    <br>
    <span style="margin-left: 1.5em;">X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 9556–9567.</span>
  </li>
  <li id="ref164" style="margin-bottom: 10px;">
    <strong>[164] Gqa: A new dataset for real-world visual reasoning and compositional question answering</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. A. Hudson and C. D. Manning. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 6700–6709.</span>
  </li>
  <li id="ref165" style="margin-bottom: 10px;">
    <strong>[165] Training verifiers to solve math word problems</strong> 
    <br>
    <span style="margin-left: 1.5em;">K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al. arXiv preprint, <a href="https://arxiv.org/abs/2110.14168" target="_blank">arXiv:2110.14168</a>, 2021.</span>
  </li>
  <li id="ref166" style="margin-bottom: 10px;">
    <strong>[166] Qwen2 technical report</strong> 
    <br>
    <span style="margin-left: 1.5em;">Q. Team. arXiv preprint, <a href="https://arxiv.org/abs/2407.10671" target="_blank">arXiv:2407.10671</a>, 2024.</span>
  </li>
  <li id="ref167" style="margin-bottom: 10px;">
    <strong>[167] Gpqa: A graduate-level google-proof q&a benchmark</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. First Conference on Language Modeling, 2024.</span>
  </li>
  <li id="ref168" style="margin-bottom: 10px;">
    <strong>[168] Measuring mathematical problem solving with the math dataset</strong> 
    <br>
    <span style="margin-left: 1.5em;">D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).</span>
  </li>
  <li id="ref169" style="margin-bottom: 10px;">
    <strong>[169] Fine-grained text style transfer with diffusion-based language models</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Lyu, T. Luo, J. Shi, T. C. Hollon, and H. Lee. arXiv preprint, <a href="https://arxiv.org/abs/2305.19512" target="_blank">arXiv:2305.19512</a>, 2023.</span>
  </li>
  <li id="ref170" style="margin-bottom: 10px;">
    <strong>[170] Benchmarking diffusion models for machine translation</strong> 
    <br>
    <span style="margin-left: 1.5em;">Y. Demirag, D. Liu, and J. Niehues. Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, 2024, pp. 313–324.</span>
  </li>
  <li id="ref171" style="margin-bottom: 10px;">
    <strong>[171] Transformers: State-of-the-art natural language processing</strong> 
    <br>
    <span style="margin-left: 1.5em;">T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al. Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 2020, pp. 38–45.</span>
  </li>
  <li id="ref172" style="margin-bottom: 10px;">
    <strong>[172] Efficient memory management for large language model serving with pagedattention</strong> 
    <br>
    <span style="margin-left: 1.5em;">W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica. Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.</span>
  </li>
  <li id="ref173" style="margin-bottom: 10px;">
    <strong>[173] Qwen3 technical report</strong> 
    <br>
    <span style="margin-left: 1.5em;">A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv et al. arXiv preprint, <a href="https://arxiv.org/abs/2505.09388" target="_blank">arXiv:2505.09388</a>, 2025.</span>
  </li>
  <li id="ref174" style="margin-bottom: 10px;">
    <strong>[174] Kimi K2: Open agentic intelligence</strong> 
    <br>
    <span style="margin-left: 1.5em;">K. Team, Y. Bai, Y. Bao, G. Chen, J. Chen, N. Chen, R. Chen, Y. Chen, Y. Chen, Y. Chen et al. arXiv preprint, <a href="https://arxiv.org/abs/2507.20534" target="_blank">arXiv:2507.20534</a>, 2025.</span>
  </li>
</ul>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AF%BB%E8%AE%BA%E6%96%87/" class="category-chain-item">读论文</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Diffusion-Model/" class="print-no-link">#Diffusion Model</a>
      
        <a href="/tags/DLMs/" class="print-no-link">#DLMs</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>A Survey on DLMs</div>
      <div>http://dbqdss.github.io/2025/10/18/读论文/A-Survey-on-DLMs/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>失去理想的獾</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年10月18日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/10/23/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI%20Notes/LLM/CS336%20Language%20Modeling%20from%20Scratch/CS336-Lec02/" title="CS336 Lec02">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CS336 Lec02</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/09/26/%E4%B8%AA%E4%BA%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/AI%20Notes/LLM/CS336%20Language%20Modeling%20from%20Scratch/CS336-Lec01/" title="CS336 Lec01">
                        <span class="hidden-mobile">CS336 Lec01</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
    
      <article id="comments" lazyload>
        
    <div id="giscus" class="giscus"></div>
    <script type="text/javascript">
      Fluid.utils.loadComments('#giscus', function() {
        var options = {"repo":"DBQDSS/DBQDSS.github.io","repo-id":"R_kgDOMbYRUA","category":"Announcements","category-id":"DIC_kwDOMbYRUM4ChNj5","theme-light":"light","theme-dark":"dark","mapping":"pathname","reactions-enabled":1,"emit-metadata":1,"input-position":"top","lang":"zh-CN","theme":"preferred_color_scheme","loading":"lazy"};
        var attributes = {};
        for (let option in options) {
          if (!option.startsWith('theme-')) {
            var key = option.startsWith('data-') ? option : 'data-' + option;
            attributes[key] = options[option];
          }
        }
        var light = 'light';
        var dark = 'dark';
        window.GiscusThemeLight = light;
        window.GiscusThemeDark = dark;
        attributes['data-theme'] = document.documentElement.getAttribute('data-user-color-scheme') === 'dark' ? dark : light;
        for (let attribute in attributes) {
          var value = attributes[attribute];
          if (value === undefined || value === null || value === '') {
            delete attributes[attribute];
          }
        }
        var s = document.createElement('script');
        s.setAttribute('src', 'https://giscus.app/client.js');
        s.setAttribute('crossorigin', 'anonymous');
        for (let attribute in attributes) {
          s.setAttribute(attribute, attributes[attribute]);
        }
        var ss = document.getElementsByTagName('script');
        var e = ss.length > 0 ? ss[ss.length - 1] : document.head || document.documentElement;
        e.parentNode.insertBefore(s, e.nextSibling);
      });
    </script>
    <noscript>Please enable JavaScript to view the comments</noscript>


      </article>
      
        
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  








    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
      <div class="col-lg-7 mx-auto nopadding-x-md">
        <div class="container custom mx-auto">
           <script src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<div class="waifu" id="waifu" style="z-index: 1000;"> <div class="waifu-tips" style="opacity: 1;"></div> <canvas id="live2d" width="280" height="250" class="live2d"></canvas> <div class="waifu-tool"> <span class="fui-home"></span> <span class="fui-chat"></span> <span class="fui-eye"></span> <span class="fui-user"></span> <span class="fui-photo"></span> <span class="fui-info-circle"></span> <span class="fui-cross"></span> </div> </div> <script src="/assets/live2d.min.js"></script> <script src="/assets/waifu-tips.js"></script> <script type="text/javascript"> initModel() </script> 
        </div>
      </div>
    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
      <i class="iconfont icon-copyright"></i>
<a href="https://github.com/DBQDSS" target="_blank" rel="nofollow noopener"><span>失去理想的獾</span></a>
|  Powered by <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
<i class="iconfont icon-love"></i>
<a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a>

<div style="font-size: 0.85rem">
  <span id="timeDate">载入天数...</span>
  <span id="times">载入时分秒...</span>
  <script src="/vvd_js/duration.js"></script>
  <br>
  <script src="//cdn.busuanzi.cc/busuanzi/3.6.9/busuanzi.min.js" defer></script>
    今日总访问量 <span id="busuanzi_today_pv">加载中...</span> 次
    今日总访客数 <span id="busuanzi_today_uv">加载中...</span> 人
    本站总访问量 <span id="busuanzi_site_pv">加载中...</span> 次
    本站总访客数 <span id="busuanzi_site_uv">加载中...</span> 人
    本页总阅读量 <span id="busuanzi_page_pv">加载中...</span> 次
  <!-- 不蒜子统计图标代码 如果更换域名记得替换Start -->
    <a href="https://www.busuanzi.cc/count.php?search=dbqdss.github.io" title="不蒜子统计" target="_blank">
        <img style="width:85px;height:25px;" src="https://www.busuanzi.cc/static/images/bsz-tongji.png" srcset="/img/loading.gif" lazyload>
    </a>
  <!-- 不蒜子统计图标代码 End -->
</div>

    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="/js/custom.js"></script>
<script src="/js/yinghua.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
